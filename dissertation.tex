\documentclass[12pt,twoside,openright]{report}

% Hyperlink references
\usepackage[pdfborder={0 0 0}]{hyperref}
% PGF diagrams
\usepackage{pgf}
\usepackage[utf8]{inputenc}\DeclareUnicodeCharacter{2212}{-}
% Page margins
\usepackage[margin=25mm]{geometry}
% Subfigures
\usepackage{caption}
\usepackage{subcaption}
% Bibliography
\usepackage[style=numeric-comp,sorting=none]{biblatex}
% Images
\usepackage{graphicx}
% Directory tree diagram
\usepackage{dirtree}
% Simple tree diagrams
\usepackage[linguistics]{forest}
\usepackage{adjustbox}
% Algorithms
\usepackage[ruled]{algorithm2e}
% Better verbatim
\usepackage{fancyvrb}
% TikZ diagrams
\usepackage{tikz}
% UK date
\usepackage[UKenglish]{babel}
\usepackage{csquotes}
% Paragraph spacing
\usepackage{parskip}
% Verbatim in caption
\usepackage{cprotect}
% Aligning equations
\usepackage{amsmath}
% Auto-column tables
\usepackage{tabularx}
% Better table lines
\usepackage{booktabs}
% Multi-row and column tables
\usepackage{multirow}
% Code syntax highlighting
\usepackage{minted}
% Multi-column environments
\usepackage{multicol}


\addbibresource{bibliography.bib}

\flushbottom

\DeclareRobustCommand{\setmetre}[2]{\ensuremath{
  \vcenter{\offinterlineskip
    \halign{\hfil##\hfil\cr
            $\scriptstyle#1$\cr
            \noalign{\vskip1pt}
            $\scriptstyle#2$\cr}
  }}\!
}

\usemintedstyle{sonicpi}

\begin{document}


\pagestyle{headings}





\chapter{Introduction} \label{introduction}

In this project, I present a novel system for generating music with
style-specific micro-timing in the Sonic Pi live coding language. I
use a probabilistic approach to control the exact timing of music written in
Sonic Pi according to realistic deviations found in music across the world. My
implementation introduces the concept of musical metre into Sonic Pi, for which
I demonstrate its successful use and then utilise it to create micro-timing. I also
perform data analysis on the micro-timing of two different styles of music and
evaluate the realism of the resulting music. This achieves the key success
criteria set out in my project proposal and one suggested extension.



\section{Motivation} \label{motivation}

Metre is a fundamental musical concept which describes the ways in which periodic events, such as beats, are grouped and divided to form a hierarchy \cite{london2012}. Micro-timing in music refers to small deviations in the timing of notes from
their exact, grid-like positions in the metre. These
deviations occur naturally in all human performances, but when they occur
systematically, they make an important contribution to what defines a musical
style. Therefore, this a very valuable feature when creating realistic music.
Many commonly used music synthesis environments (such as Sonic Pi) lack the
ability to control micro-timing during playback beyond simple jazz swing effects.
By implementing this, I aim to give Sonic Pi composers the ability to produce
music in certain styles more realistically. It also allows for further
creativity, such as by applying the micro-timing strategies of one style to
music from another style.

Musical live coding is a way of creating and performing music by writing
and modifying code in real time \cite{magnusson2011}. This was an interesting area to focus on for
this project because the nature of live coding allows for creative applications
of micro-timing, such as adjusting micro-timing throughout a performance.

The second major component of my project was the data analysis I performed
on recordings of music from two case study styles. The motivation for this was
to be able to use my extended Sonic Pi language to generate music from these styles with
realistic micro-timing.



\section{Related work} \label{related_work}

The main areas of related work to this project are the analysis of micro-timing in styles of music, generating music with micro-timing, and representations of metre in live coding languages.

The micro-timing of different musical styles has been analysed extensively in related work. One example is Bengtsson and Gabrielsson's analysis of Viennese waltz recordings, which demonstrated a long-short-medium pattern in the length of its beats \cite{bengtsson1977}. Friberg and Sundstr√∂m performed an analysis of the long-short pattern of swung eighth notes in jazz music \cite{friberg2002}. Finally, Polak has done much work on the micro-timing of jembe music from Mali (see Section~\ref{case_studies}) \cite{polak2010,london2017,jacoby2021}.

Creating music with synthetic micro-timing has been explored as part of music
information retrieval (MIR) for a long time. This is often attempting to model
human-like expressive timing \cite{bilmes1993}. For example, Oore et al.\ used an
LSTM-based recurrent neural network to generate piano music with expressive
timing and dynamics \cite{oore2020}. However, there is little academic research on implementing user-defined, systematic micro-timing as in this project. A commercial product which has this functionality is the FlexGroove\footnote{\url{https://www.ableton.com/en/packs/flexgroove/}} tool for Max For Live. It gives the user
control over swing and other micro-timing deviations, including a probabilistic
element.

Most live coding languages lack a full hierarchical representation of musical
metre (Section~\ref{metre_background}), typically only representing beats and time signatures.
For example, the commercial Max/MSP language uses its \verb'transport' object to allow
access to bar and beat numbers for the current time signature. Gibber emphasises a \setmetre{4}{4} time signature by default, and allows users to trigger music to play at the beginning of a bar \cite{roberts2012}. An exception is McLean's open-source Tidal Cycles which uses a cyclic notion of time that can be subdivided to achieve more complex hierarchies \cite{mclean2010}. Tidal also has a \verb'swingBy' function\footnote{\url{https://tidalcycles.org/docs/patternlib/tour/time/}}, which
implements a simple jazz swing micro-timing effect.

In summary, though there has been much research into the analysis of
micro-timing in different musical styles, and the application of expressive
timing to computer-generated music, implementations of style-specific
micro-timing as part of music software are rare. Additionally, musical metre is
a concept often underrepresented in live coding. With this project, I aimed to
address this with my implementation for Sonic Pi.





\chapter{Preparation} \label{preparation}

This chapter introduces the core theories and background material necessary
to understand the project. I explain some key concepts in music theory and
live coding, and give an overview of the case study musical styles. Next, I describe the starting point of this project, expand upon the requirements from
the project proposal, and conclude with an outline of the software engineering
techniques used throughout.



\section{Background} \label{background}


\subsection{Musical metre} \label{metre_background}



\subsubsection{Micro-timing} \label{micro-timing_background}

Every note in a piece of music occurs at a particular metrical position, but
music performed by humans often deviates from these mechanically even timings
\cite{london2012}. The ways in which these deviations occur contribute significantly
to what we call musical style by producing \emph{micro-timing} effects.

Different
musical styles have different micro-timing strategies. A common example is the
long-short pattern in swing rhythms, where every other eighth note (the first
division level) is slightly delayed. This timing information is typically lost
in music notation, so musicians have to use
their knowledge and experience of the style to inform their performance of the
micro-timing. This is formalised in Justin London's Many Meters Hypothesis, where he also argues that these micro-timing effects actually form part of the metre itself \cite{london2012}.

A metre without any micro-timing is said to be \emph{isochronous},
which means the metrical events at each level are equally spaced in time.


\subsection{Live coding} \label{live_coding_background}

Musical live coding is a way of creating and performing music by writing and
modifying code in real time. The performer typically projects their code onto a
screen for an audience to follow along with \cite{magnusson2011}. The liveness of the
performance is important -- it is argued that the musician must directly
interact with the running algorithms for it to be truly considered live coding
\cite{collins2011}. If it were not for this, there would be little difference between live coding and playing a prerecorded piece of music, because much of the performance element would be lost.

Sonic Pi is a popular live coding language and IDE created by Sam Aaron, which
was designed as an educational tool for teaching programming in schools \cite{aaron2013}. It
implements its own domain-specific language written in Ruby, using the
SuperCollider sound synthesis server to produce sounds. A simple
Sonic Pi program might look like the following:

\begin{minted}{ruby}
play :C4
sleep(1)
play :C5
sleep(0.5)
play :C4
\end{minted}

This plays the note C4 on the current synthesiser, sleeps for one beat, plays C5,
sleeps for half a beat, then plays C4 again.

A key component of the Sonic Pi language is the \verb'live_loop', which executes a
block of code in a loop in a new thread. This multi-threading allows multiple instruments/parts to be played together. To maintain correct timing in the presence
of multiple threads and long execution time, Sonic Pi uses sophisticated
``virtual time'' functionality behind-the-scenes \cite{aaron2014}.

Another Sonic Pi command that is important to this project is \verb'time_warp'. This executes a given block of code with a specified timing shift. Positive shift values execute the block with a delay, and negative values execute it ahead of time.

One drawback of Sonic Pi is that it currently does not have a built-in notion of
metre or style-specific micro-timing, which is what this project addresses.


\subsection{Case studies} \label{case_studies}

This project uses two different styles of music as case studies for evaluation,
both of which have well-known micro-timing characteristics. The first is jembe
drum music from Mali, which has highly regular micro-timing, and the second is
Viennese waltz music, which offers a Western comparison.

Jembe ensemble music is a style of music involving a small group of drummers
which originated in West Africa. The primary drum is the eponymous jembe, which
is a goblet-shaped drum played with the bare hands (shown in Figure~\ref{fig:jembe_photo}). This is usually accompanied
by the dundun, which is a cylindrical drum played with a stick \cite{polak2010}. In a
traditional jembe trio, there are two jembe players and one dundun player, with
Jembe 1 having a lead role, Jembe 2 playing a simple accompaniment, and Dundun
playing a varying pattern that is characteristic of each piece of music
\cite{jacoby2021}. Different playing techniques produce various kinds of sound
(timbre) on each drum, which gives rise to the melodic qualities of the music
\cite{polak2010}.

Jembe music is an ideal case study for this project because it has a highly
consistent micro-timing strategy \cite{polak2010}, and Malian drummers have been shown to
have one of the lowest levels of timing variability between performers in the
world \cite{clayton2020}. Compared with other styles of music, jembe music is
relatively constrained in terms of its pitch, timbre, and number of instruments,
which allows for a clear focus on timing. The strength of existing research \cite{polak2010,london2017,jacoby2021} into the micro-timing of jembe music is another reason why it makes a good case study.

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.5\linewidth]{figures/polak_ex1.jpg}
    \caption{Left: Dundun (Madu Jakite). Centre: Jembe 1 (Sedu Balo). Right: Jembe 2 (Drissa Kone). Reproduced from Polak \cite{polak2010}.}
    \label{fig:jembe_photo}
\end{figure}

The Viennese waltz is a style of music intended for ballroom dancing. It is a
fast waltz in \setmetre{3}{4} at around 180 beats per minute (bpm) and is performed by a
classical Western orchestra. It has a characteristic short-long-medium
micro-timing pattern for the length of its beats \cite{bengtsson1974,bengtsson1977}.




\chapter{Implementation} \label{implementation}

The implementation of this project consists of a few main components.

The first is the addition of the micro-timing functionality into Sonic Pi.
This consists of implementing musical metre, new commands to play music
within a metrical context, the style-specific micro-timing itself, and a method
of multi-threaded synchronisation. The second main component is the data
analysis which is used to generate realistic micro-timing for musical styles.
This is done primarily for Malian jembe, but also for Viennese waltz in
preparation for the evaluation and to check generalisability. The final component is a 
pair of converters between the MusicXML file format and my extended Sonic Pi language, 
also implemented for use in the evaluation.

This chapter details the data structures, algorithms, and approaches used to
implement these features.



\section{Metre} \label{metre_implementation}

Implementing a concept of musical metre within Sonic Pi was a crucial step
towards adding micro-timing functionality. Without it, the system has no way of
knowing where each note falls within the metrical cycle, and therefore no way of
knowing what timing adjustment it should apply. This section describes the way
the metrical hierarchy has been implemented as a tree data structure, and the
main algorithms which act on it. I then go on to describe the Bar class as a
representation of a single metrical cycle.


\subsection{Metrical hierarchy as trees} \label{metrical_hierarchy}

A tree data structure was chosen as a more descriptive representation for the
hierarchy information within a metre. This is a common way to depict metre;
Forth provides a detailed mathematical treatment of trees used in this context
\cite{forth2012}. To ensure an implementation that works well as part of a
programming language, I used the popular \verb'music21' Python library as a basis for
designing my representation \cite{ariza2010}.

The tree structure is implemented by the MetreLeaf and MetreTree classes (see
Sections~\ref{metreleaf} and~\ref{metretree} for more details). Figure~\ref{fig:tree_object_hierarchy} shows the default tree structure formed by these objects and their durations
for both a \setmetre{4}{4} time signature and an unconventional, pathological example metre. Note how the duration of a parent node is the sum of the durations of its children. Figure~\ref{fig:unconventional_metre_notation} shows some example music for the unconventional metre and how the grouping of notes in Western music notation is directed by the metrical hierarchy.

\begin{figure}[ht]
    \centering
    \begin{subfigure}{\linewidth}
        \centering
        \resizebox{\linewidth}{!}{
            \begin{forest}
                for tree=draw,
                [{\small \bfseries Metre \\ $d=1$}
                    [{\small \bfseries MetreTree \\ $d=\frac{1}{4}$}
                        [{\small \bfseries MetreLeaf \\ $d=\frac{1}{8}$}]
                        [{\small \bfseries MetreLeaf \\ $d=\frac{1}{8}$}]
                    ]
                    [{\small \bfseries MetreTree \\ $d=\frac{1}{4}$}
                        [{\small \bfseries MetreLeaf \\ $d=\frac{1}{8}$}]
                        [{\small \bfseries MetreLeaf \\ $d=\frac{1}{8}$}]
                    ]
                    [{\small \bfseries MetreTree \\ $d=\frac{1}{4}$}
                        [{\small \bfseries MetreLeaf \\ $d=\frac{1}{8}$}]
                        [{\small \bfseries MetreLeaf \\ $d=\frac{1}{8}$}]
                    ]
                    [{\small \bfseries MetreTree \\ $d=\frac{1}{4}$}
                        [{\small \bfseries MetreLeaf \\ $d=\frac{1}{8}$}]
                        [{\small \bfseries MetreLeaf \\ $d=\frac{1}{8}$}]
                    ]
                ]
            \end{forest}
        }
        \caption{Object hierarchy for \setmetre{4}{4}}
        \label{fig:tree_object_hierarchy_4_4}
    \end{subfigure}
    \\[1.5ex]
    \begin{subfigure}{\linewidth}
        \centering
        \resizebox{0.8125\linewidth}{!}{
            \begin{forest}
                for tree=draw,
                [{\small \bfseries Metre \\ $d=\frac{11}{8}$}
                    [{\small \bfseries MetreTree \\ $d=\frac{1}{4}$}
                        [{\small \bfseries MetreLeaf \\ $d=\frac{1}{8}$}]
                        [{\small \bfseries MetreLeaf \\ $d=\frac{1}{8}$}]
                    ]
                    [{\small \bfseries MetreTree \\ $d=\frac{1}{4}$}
                        [{\small \bfseries MetreLeaf \\ $d=\frac{1}{16}$}]
                        [{\small \bfseries MetreLeaf \\ $d=\frac{3}{16}$}]
                    ]
                    [{\small \bfseries MetreLeaf \\ $d=\frac{1}{8}$}]
                    [{\small \bfseries MetreTree \\ $d=\frac{3}{4}$}
                        [{\small \bfseries MetreLeaf \\ $d=\frac{1}{4}$}]
                        [{\small \bfseries MetreTree \\ $d=\frac{1}{2}$}
                            [{\small \bfseries MetreLeaf \\ $d=\frac{5}{16}$}]
                            [{\small \bfseries MetreLeaf \\ $d=\frac{3}{16}$}]
                        ]
                    ]
                ]
            \end{forest}
        }
        \caption{Object hierarchy for a pathological \setmetre{11}{8} metre}
        \label{fig:tree_object_hierarchy_unconventional}
    \end{subfigure}
    \caption{Two examples of how MetreTree and MetreLeaf objects are nested to construct metrical hierarchies for two different metres. The total duration $d$ of each node is also displayed, and the duration of a parent node is the sum of the durations of its children.}
    \label{fig:tree_object_hierarchy}
\end{figure}

\begin{figure}[ht]
    \centering
    \includegraphics[width=\linewidth]{figures/hierarchy_notation_v2.pdf}
    \caption{One bar of music using the metre in Figure~\ref{fig:tree_object_hierarchy_unconventional}. Western music notation shows the hierarchy by the way it groups notes.}
    \label{fig:unconventional_metre_notation}
\end{figure}

The tree data structures can only have a finite depth, whereas metrical hierarchies are infinitely deep (otherwise we would not be able to represent notes with very short durations). Therefore, we have to make assumptions about the structure of the hierarchy for levels deeper than those defined by the data structure. In this case, we assume that each MetreLeaf is divided into two to get the next level.


\subsection{MetreLeaf class} \label{metreleaf}

A MetreLeaf object is the leaf node of the metrical tree structure. It has an
instance variable \verb'fraction' which represents the duration of the
MetreLeaf as a fraction of a whole note. This is stored as a Rational -- a Ruby object representing a rational number as a simplified fraction. For
example, a leaf node with the duration of one quarter note will have the value $\frac{1}{4}$.

The class contains a \verb'subdivide()' method, which divides the MetreLeaf by two a
given number of times, $s$. It returns a new MetreTree with $2^s$ MetreLeaf children, each of value $f/2^s$ where $f$ is the fraction of the original MetreLeaf.


\subsection{MetreTree class} \label{metretree}

A MetreTree object represents the hierarchical tree or subtree of a metre. The
instance variable \verb'sequence' is an ordered list representing this node's children and contains any combination of
MetreLeaf objects and other MetreTree objects. For
example, the previous example hierarchy in Figure~\ref{fig:tree_object_hierarchy_4_4} could also be written in list form as:
\[\left[\left[\frac{1}{8},\frac{1}{8}\right],\left[\frac{1}{8},\frac{1}{8}\right],\left[\frac{1}{8},\frac{1}{8}\right],\left[\frac{1}{8},\frac{1}{8}\right]\right]\]
Each list is a
MetreTree, and each fraction is a MetreLeaf. The MetreTree class contains
several methods for manipulating and extracting information from the metrical
hierarchy it represents. The two most important of these are explained in more
detail below.

\subsubsection{Getting metrical levels} \label{get_level}

Recall that the depth levels of a metrical hierarchy are called \emph{metrical levels} -- the basis of which is the beat level. The beat is divided to get \emph{division levels}, and grouped to get \emph{multiple levels}. A useful transformation on a MetreTree is to flatten the tree structure to a specified depth. This allows us to access the sequence of events at a given metrical level.

This is implemented by the \verb'get_level()' method, which returns a flat MetreTree at
a given metrical level $l$. A flat MetreTree is defined as one whose children are
only MetreLeafs, meaning there is no hierarchy (e.g.\ $\left[\frac{1}{8},\frac{1}{8},\frac{1}{8},\frac{1}{8},\frac{1}{8},\frac{1}{8},\frac{1}{8},\frac{1}{8}\right]$). Despite still being stored as a MetreTree object, it no longer represents a full metrical hierarchy, just one level of it.

This method is split into two algorithms:
\begin{itemize}
    \item \verb'get_division_level()' computes the sequence for division levels ($l>0$) and the beat level ($l=0$).
	\item \verb'get_multiple_level()' computes a possible sequence for multiple levels ($l<0$).
\end{itemize}

Algorithm~\ref{alg:getDivisionLevel} shows the \verb'get_division_level()' method. For each child in the
sequence list, if it is a MetreTree, the method is recursively called until the
base case of $l=0$ is reached. At this point, all the children of that node are
combined into one MetreLeaf equal to the sum of their durations. If the child is
instead a MetreLeaf, it is subdivided $l$ times to reach the desired metrical
level.

\begin{algorithm}[H]
    \SetKw{KwForIn}{in}
    \SetKwFunction{GetDivLevel}{get\_division\_level}

    \caption{get\_division\_level()}
    \KwIn{Target metrical level, $l$}
    \KwOut{New flat MetreTree at level $l$}
    \BlankLine

    new\_sequence $\gets$ empty list\;
    \ForEach{child \KwForIn @sequence}{
        \eIf{child is a MetreLeaf}{
            \eIf{$l>0$}{
                append (child subdivided $l$ times) to new\_sequence\;
            }{
                append child to new\_sequence\;
            }
        }{
            \tcp{child is a MetreTree}
            \eIf{$l>0$}{
                $r \gets$ recursive call to \GetDivLevel{$l-1$} method on child\;
                append $r$ to new\_sequence\;
            }{
                \tcp{Base of recursion}
                $m \gets$ combine all children of child into one MetreLeaf\;
                append $m$ to new\_sequence\;
            }
        }
    }
    \Return{new MetreTree made from new\_sequence}
    \label{alg:getDivisionLevel}
\end{algorithm}
\pagebreak

The \verb'get_multiple_level()' algorithm performs an estimate of the structure of
higher metrical levels by clustering nodes together. It is an estimate because this information is not in the
MetreTree's representation of the metre, so is just one possibility for the higher structure. The algorithm recursively clusters nodes until
the desired metrical level $l$ is reached. The number of nodes combined in each cluster is
determined by the smallest prime factor of the number of nodes at the level below. For
example, if level $l+1$ has four nodes, they will be clustered in groups of two. If it
has nine nodes, they will be clustered in groups of three.

An important consideration when implementing the \verb'get_level()' method was to
maximise its efficiency, because it is called often -- at least once per note. The
running time of the algorithm is $O(\left\lvert l\right\rvert)$, where $\left\lvert l\right\rvert$ is the number of MetreLeafs in the
hierarchy. To improve the efficiency, I implemented a cache of metrical levels for each
MetreTree object to store expensive computations for later reuse.

Some examples of the output of \verb'get_level()' for the following pathological hierarchy (from Figure~\ref{fig:tree_object_hierarchy_unconventional}) are shown in Table~\ref{table:get_level}:
\[
    \left[
        \left[\frac{1}{8},\frac{1}{8}\right],
        \left[\frac{1}{16},\frac{3}{16}\right],
        \frac{1}{8},
        \left[\frac{1}{4},\left[\frac{5}{16},\frac{3}{16}\right]\right]
    \right]
\]

\begin{table}[ht]
    \centering
    \renewcommand{\arraystretch}{2.0}
    \begin{tabular}{|c|c|}
        \hline
        $l$     & \verb'get_level'$(l)$ \\
        \hline
        $-2$    & $\displaystyle \left[ \frac{11}{8} \right]$ \\
        $-1$    & $\displaystyle \left[ \frac{1}{2},\frac{7}{8} \right]$ \\
        $0$     & $\displaystyle \left[ \frac{1}{4},\frac{1}{4},\frac{1}{8},\frac{3}{4} \right]$ \\
        $1$     & $\displaystyle \left[ \frac{1}{8},\frac{1}{8},\frac{1}{16},\frac{3}{16},\frac{1}{16},\frac{1}{16},\frac{1}{4},\frac{1}{2} \right]$ \\
        $2$     & $\displaystyle \left[ \frac{1}{16},\frac{1}{16},\frac{1}{16},\frac{1}{16},\frac{1}{32},\frac{1}{32},\frac{3}{32},\frac{3}{32},\frac{1}{32},\frac{1}{32},\frac{1}{32},\frac{1}{32},\frac{1}{8},\frac{1}{8},\frac{5}{16},\frac{3}{16} \right]$ \\ [1ex]
        \hline
    \end{tabular}
    \renewcommand{\arraystretch}{1.0}
    \cprotect\caption{Examples of the output of \verb'get_level'$(l)$ at different metrical levels $l$ for the hierarchy $[[1/8,1/8],[1/16,3/16],1/8,[1/4,[5/16,3/16]]]$. Note how level $l=-1$ is formed by the clustering of level $l=0$.}
    \label{table:get_level}
\end{table}

\subsubsection{Getting exact metrical events} \label{metrical_level_indices}

The second key method in the MetreTree class is \verb'metrical_level_indices()'. We define an \emph{offset} as a position in the metric cycle represented as the number of quarter lengths (the length of one quarter note) since the beginning of the cycle. For a
given offset, this algorithm finds any metrical events
which occur exactly at this offset, and returns their index.

Consider the example shown in Figure~\ref{fig:metrical_level_indices_example}. Offset $x$
occurs on the first event of all three levels, so the function would return $L_0(x)=L_1(x)=L_2(x)=0$, where $L_l(x)$ is the index of an event at level $l$ that offset $x$ occurs on. Offset $y$ occurs only on the last event of Level 1 and the second-to-last event of Level 2, so the function would return $L_1(y)=3, L_2(y)=6$.

This method is important because it
is used later to determine which micro-timing probability distributions should
be applied to a note at a given offset (see Section~\ref{applying_micro-timing}).

\begin{figure}[ht]
    \centering
    \resizebox{\linewidth}{!}{
        \begin{adjustbox}{valign=t}
            \begin{forest}
                for tree={no edge},
                [, [Level 0 [Level 1 [Level 2]]]]
            \end{forest}
        \end{adjustbox}\qquad
        \begin{adjustbox}{valign=t}
            \begin{forest}
                for tree={calign=first},
                [,phantom,name=Phantom1
                    [{$\frac{1}{4}$},name=First4
                        [{$\frac{1}{8}$}
                            [{$\frac{1}{16}$}]
                            [{$\frac{1}{16}$}]
                        ]
                        [{$\frac{1}{8}$}
                            [{$\frac{1}{16}$}]
                            [{$\frac{1}{16}$}]
                        ]
                    ]
                ]
                \node(xNode)[red] at (First4 |- Phantom1) {$x$};
                \draw[->,red] (xNode) to (First4);
            \end{forest}
        \end{adjustbox}\qquad
        \begin{adjustbox}{valign=t}
            \begin{forest}
                for tree={calign=first},
                [,phantom,name=Phantom2
                    [{$\frac{1}{4}$}
                        [{$\frac{1}{8}$}
                            [{$\frac{1}{16}$}]
                            [{$\frac{1}{16}$}]
                        ]
                        [{$\frac{1}{8}$},name=Last8
                            [{$\frac{1}{16}$}]
                            [{$\frac{1}{16}$}]
                        ]
                    ]
                ]
                \node(yNode)[red] at (Last8 |- Phantom2) {$y$};
                \draw[->,red] (yNode) to (Last8);
            \end{forest}
        \end{adjustbox}\qquad
    }
    \caption{An example metrical hierarchy for \setmetre{2}{4} showing which metrical events at each level (if any) offsets $x$ and $y$ occur on. $x$ occurs on the first event in all three levels. $y$ only occurs exactly on an event in Levels 1 and 2 -- namely events with indices 3 and 6, respectively.}
    \label{fig:metrical_level_indices_example}
\end{figure}


\subsection{Metre class} \label{metre_class}

The Metre class is a subclass of MetreTree which acts as a wrapper for the
hierarchy stored in a MetreTree. It implements functionality allowing a user to
specify a metre by a time signature string (e.g.\ \verb|'4/4'|) for common metres, or a
nested list of Rationals representing the hierarchy (e.g.\ \verb'[[1/8r,1/8r],[1/16r,3/16r],1/8r,[1/4r,[5/16r,3/16r]]]'). It also has a method which
converts a duration in quarter lengths to Sonic
Pi beats.


\subsection{Bar class} \label{bar_class}

A bar\footnote{Known as a \emph{bar} in British English, and a \emph{measure} in American English.} is a common term used in Western music theory for a single
metric cycle\footnote{A more precise definition would account for \emph{hypermetre}, where bars occur at the beat level \cite{neal2000}, but the simple definition is sufficient for our purposes.}. The Bar class is a representation of this, and each instance of it
has an associated metre. A Bar object is responsible for:
\begin{itemize}
	\item Keeping track of the playback position through the cycle (the \verb'current_offset' variable).
	\item Converting a note length given as a metrical level and a duration into
quarter lengths.
	\item Checking if a note or rest fits in the remaining time in the cycle, and
updating the bar's playback position accordingly.
\end{itemize}

A note's length is specified as a metrical level and a duration. The duration
is in units of an event at the specified metrical level, and acts as a multiplier.
For example, if a note's length is defined as $(l,d)=(0,3)$, its unit length is the duration of an event at level $l=0$, and it lasts for $d=3$ of these units.

The \verb'add_note()' method handles checking if a note fits into the bar, as shown in
Algorithm~\ref{alg:add_note}. If a note cannot fit into the bar's remaining time,
an exception is raised. This ensures the bar obeys the duration of its metre.

\begin{algorithm}[h]
    \SetKwFor{RepeatTimes}{repeat}{times do}{end}
    \SetKw{Raise}{raise}

    \caption{add\_note()}
    \KwIn{Metrical level, $l$}
    \KwIn{Duration, $d$}
    \KwIn{Total metre duration (in quarter lengths), $q$}
    \KwIn{Bar's current playback position, @current\_offset}
    \BlankLine

    new\_offset $\gets$ @current\_offset\;
    $M \gets$ metre flattened to level $l$\;
    \RepeatTimes{$d$}{
        \eIf{new\_offset $\geq q$}{
            \Raise CannotFitNoteException\;
        }{
            $n \gets$ length of active event in $M$ at new\_offset\;
            new\_offset $\gets \textnormal{new\_offset} + n$\;
        }
    }
    @current\_offset $\gets$ new\_offset\;
    \label{alg:add_note}
\end{algorithm}



\section{Playing music} \label{playing_music}

Now that the framework of musical metre has been established, we can look at how
this is used to play music. This section describes the new Sonic Pi
commands I have implemented for controlling metre, and adding notes and rests.
Figure~\ref{fig:sonicpi_language_comparison} shows a comparison between the original Sonic Pi commands (left), my
alternative commands (centre), and traditional Western music notation (right)
for a single bar of \setmetre{4}{4}.
\newpage

\begin{figure}[ht]
    \centering
    \begin{subfigure}[b]{0.2\textwidth}
        \centering
        \begin{minted}{ruby}
play :C4
sleep(1)
play :E4
sleep(1)
play :G4
sleep(0.5)
play :E4
sleep(0.5)
play :C4
        \end{minted}
        \caption{Old}
    \end{subfigure}
    %
    \begin{subfigure}[b]{0.4\textwidth}
        \centering
        \begin{minted}{ruby}
use_metre '4/4'

bar do
    add_note :C4, 0, 1
    add_note :E4, 0, 1
    add_note :G4, 1, 1
    add_note :E4, 1, 1
    add_note :C4, 0, 1
end
        \end{minted}
        \caption{New}
    \end{subfigure}
    %
    \begin{subfigure}[b]{0.3\textwidth}
        \centering
        \includegraphics[width=\linewidth]{figures/sonic_pi_comparison.pdf}
        \caption{Notation}
    \end{subfigure}
    \cprotect\caption{A single bar of music represented by the original Sonic Pi syntax, my new metre commands, and traditional Western music notation. Note how the original Sonic Pi syntax loses information about the metre. The second and third arguments to \verb'add_note' are the metrical level and duration.}
    \label{fig:sonicpi_language_comparison}
\end{figure}


\subsection{Metre commands} \label{metre_commands}

A metre is created with the \verb'use_metre' or \verb'with_metre' commands. These have been
designed to match the syntax of existing Sonic Pi commands such as \verb'use_fx' and \verb'with_fx'. Sonic Pi uses multiple threads of execution to allow different musical parts to play simultaneously. Each of these commands operates only within its current thread. See Section~\ref{multi-threaded_synchronisation} for more on multi-threading.

\begin{itemize}
	\item \verb'use_metre('$m$\verb')' changes the current thread's metre to $m$ (using a thread-local variable).
	\item \verb'with_metre('$m$\mintinline{ruby}|) bar do ... end| executes a block of user code with a specified metre $m$, then restores the thread's original metre.
\end{itemize}

The \mintinline{ruby}|bar do ... end| command creates a new Bar object, stores this to a
thread-local variable, then executes a block of user code. If the Bar object
still has space remaining after the block has run, the function then sleeps for
the appropriate time.


\subsection{Sound commands} \label{sound_commands}

There are three main sound commands: \verb'add_note', \verb'add_sample', and \verb'add_rest'. A
user can use \verb'add_note' to play a note on the current synthesiser. A note is
defined as a pitch (how high or low it sounds) and a length. The pitch is
specified as in Sonic Pi's \verb'play' command, such as by a note name and octave (C4)
or a MIDI note (60). A list of pitches will sound together as a chord. The
length is given as a metrical level and a duration, as defined in Section~\ref{bar_class}.

The \verb'add_note' command works as follows:
\begin{enumerate}
    \item Gets the current Bar object from the thread-local variables.
    \item Calls the Bar's \verb'add_note()' method to check if the note will fit into the current bar.
    \item Passes the note pitch to Sonic Pi's \verb'play' function which creates the sound.
    \item Sleeps for the duration of the note.
\end{enumerate}

I have also implemented some shorthand commands such as \verb'add_quarter' and
\verb'add_whole' which act as wrappers around calls to \verb'add_note'. This was done to
provide an easy-to-use alternative to the metrical level notation for users who
are less familiar with Western music theory. This was especially important as
Sonic Pi was designed to be accessible to schoolchildren \cite{aaron2013}.

The final two sound commands are \verb'add_sample' and \verb'add_rest'. These function the
same as \verb'add_note', except \verb'add_sample' plays an audio sample and \verb'add_rest' simply sleeps.



\section{Micro-timing} \label{micro-timing_implementation}

In order to add micro-timing functionality to my implementation, I first needed
a way of representing and storing the micro-timing information for different
musical styles. This information is represented by probability distributions for
each event in a metric cycle, describing how early or late each event should
occur. In this section, I describe how the
Style class stores the micro-timing information for a musical style, and how
this is used to apply micro-timing to a user's music.


\subsection{Style class} \label{style_class}

The Style class stores micro-timing values as a Hash from metrical levels (0, 1,
etc.) to lists containing one probability distribution object for each event at
that level. It will accept any distribution object which has a \verb'sample()' method,
such as my NormalDistribution class described below.

For example, a Style object with the Hash shown below defines micro-timing only for metrical level
$l=0$ (the beat level) for a metre with three beats. The first beat is played exactly on time,
with no variance. The second beat is early, with normal distribution $(\mu,\sigma)=(-0.3,0.01)$.
The third beat is very slightly late, with normal distribution $(\mu,\sigma)=(0.001,0)$.

\begin{minted}{ruby}
{
    0 => [
        NormalDistribution.new(0,0),
        NormalDistribution.new(-0.3, 0.01),
        NormalDistribution.new(0.001,0)
    ]
}
\end{minted}

The class also has a test to see if the style is compatible with a given metrical
hierarchy. A Style is defined as being compatible with a MetreTree if, for each
metrical level defined in the Style, it has exactly one distribution for each
metrical event at that level in the MetreTree.

\subsubsection{NormalDistribution class} \label{normal_distribution}

I have implemented a NormalDistribution class, which is initialised with a mean $\mu$ and standard deviation
$\sigma$, and has a method for generating random samples.

Samples are generated using the
Box-Muller transform \cite{box1958}, which transforms a pair of uniform random
samples into a pair of normally distributed samples. Sonic Pi's random number
generator is used for the uniform random samples. This is because it generates a
deterministic, repeatable sequence of pseudorandom numbers, which means the output of
a Sonic Pi program sounds the same each time it is run \cite{aaron2016}.


\subsection{Applying micro-timing} \label{applying_micro-timing}

When a user sets a metre with the \verb'use_metre' or \verb'with_metre' commands, they can
specify an optional second argument as either a Style object or the name of a
style to be looked up, as shown below. This causes all music played with that metre to use the micro-timing of the chosen style.
\begin{minted}{ruby}
use_metre '12/8', :jembe
use_metre '4/4', Style.new("example", {0 => [...]})
\end{minted}

At the start of each new bar, the Metre object samples new values from the
Style's probability distributions. When a note is played inside the bar, the
\verb'add_note' command requests the timing shift that should be applied to the note
from the Metre. To calculate this, the Metre object calls its
\verb'metrical_level_indices()' method (see Section~\ref{metrical_level_indices}) to determine which timing
values from each level to use. The individual timing contributions of each
metrical level are summed to produce an overall timing shift for the
note. A positive value means the note should be played slightly late; a negative
value means slightly early. This is returned to \verb'add_note' which then uses Sonic Pi's \verb'time_warp' function (see Section~\ref{live_coding_background}) to adjust the timing of the call to \verb'play'.

For example, if the sampled timings, $T_l$, for each level, $l$, are:
\begin{equation*}
    \begin{split}
        T_0 &= [0,0.1] \\
        T_1 &= [0.03,0,0,-0.02]
    \end{split}
\end{equation*}
and the metrical level indices, $L_l$, for each level, $l$, are:
\begin{equation*}
    \begin{split}
        L_0 &= 1 \\
        L_1 &= 3
    \end{split}
\end{equation*}
then the timing shift, $t$, would be calculated by:
\begin{equation*}
    \begin{split}
        t &= \sum_{i \in T.\mathrm{keys}} T_i[L_i] \\
        &=T_0[L_0]+T_1[L_1] \\
        &=0.1+(-0.02) \\
        &=0.08
    \end{split}
\end{equation*}
Therefore, the note will be played 0.08 quarter lengths late.



\section{Multi-threaded synchronisation} \label{multi-threaded_synchronisation}

An important part of producing music in Sonic Pi is the ability to have multiple
instruments/parts playing simultaneously. This is accomplished by having
separate threads of execution. Aaron et al.'s previous work on Sonic Pi's
temporal semantics ensures music in separate threads remains in time \cite{aaron2014}.
Because of the probabilistic nature of my micro-timing implementation, each
thread needs to have the same set of randomly generated timing values to remain
perfectly synchronised. Otherwise, notes in different threads that are supposed
to sound at the same moment might not do so. To tackle this, I implemented a
SynchronisedMetre class which generates new random timing values exactly once
per bar.


\subsection{SynchronisedMetre class} \label{synchronised_metre}

The SynchronisedMetre class is a subclass of Metre which is designed to control
the synchronisation of all the notes contained in the metre. Its main functions,
in addition to those of the Metre class, are to:
\begin{itemize}
	\item Set a Style to use for generating micro-timing values.
	\item Sample from the Style's probability distributions to get timing values.
	\item Compute the total timing shift to be applied to a note at a given offset.
	\item Ensure all Bar objects which use it remain synchronised and all have the same set of timing values.
\end{itemize}


\subsection{Bar number synchronisation} \label{bar_number_synchronisation}

To make sure Bar objects in separate threads all have the same micro-timing
values, and that these values are regenerated exactly once per cycle, I
synchronise the Bars on the current `bar number'. This is a counter of the
number of bars that have occurred since the beginning of playback. The behaviour
of the Bar class is as follows:

\begin{minted}{ruby}
class Bar
    def initialize()
        previous_bar_number = __thread_locals.get(:sonic_pi_bar_number)
        if previous_bar_number
            current_bar_number = @metre.request_bar(previous_bar_number + 1)
        else
            current_bar_number = @metre.request_bar(0)
        end
        __thread_locals.set(:sonic_pi_bar_number, current_bar_number)
    end
end
\end{minted}

If this is the first bar in the thread, the Bar object will request bar number 0
from the metre, otherwise it will request the next bar number in sequence. The
metre returns the actual current bar number, which is then stored in the thread
locals for the next bar to use.

\begin{minted}{ruby}
class SynchronisedMetre < Metre

    def initialize()
        @timings = recalculate_timings()
        @current_bar_number = 0
        @mutex = Mutex.new()
    end

    def request_bar(requested_bar_number)
        @mutex.synchronise do
            if requested_bar_number > @current_bar_number
                @timings = recalculate_timings()
                @current_bar_number = requested_bar_number
            end
        end
        return @current_bar_number
    end
end
\end{minted}

The SynchronisedMetre's \verb'request_bar()' method determines the correct current bar
number. If the requested number is greater than its internal current number,
then it must be the start of a new bar. So, the timings are recalculated, and
its current bar number is updated. If the requested number is old, then it just
returns the current number. The method is synchronised on a mutex to avoid any
race conditions which might cause bars to get different timing values.

\usetikzlibrary{positioning}
\begin{figure}[ht]
    \centering
    \begin{tikzpicture}[node distance=4]
        \node[] (Metre) {SynchronisedMetre};
        \node[right = of Metre] (Thread1) {Thread 1};
        \node[right = of Thread1] (Thread2) {Thread 2};
        \node[below of=Metre, node distance=7cm] (Metre_ground) {};
        \node[below of=Thread1, node distance=7cm] (Thread1_ground) {};
        \node[below of=Thread2, node distance=7cm] (Thread2_ground) {};
        %
        \draw (Metre) -- (Metre_ground);
        \draw (Thread1) -- (Thread1_ground);
        \draw (Thread2) -- (Thread2_ground);
        \draw[->] (Thread1 |- 0,-1) -- node[above,sloped,midway]{requestBar(0)} node[at start,right]{0} node[at end,left]{0} (Metre |- 0,-2);
        \draw[->] (Metre |- 0,-2) -- node[above,sloped,midway]{$b=0$} node[at end,right]{0} (Thread1 |- 0,-2.5);
        \draw[->] (Thread1 |- 0,-3) -- node[above,sloped,midway]{requestBar(1)} node[at end,left]{1} (Metre |- 0,-4);
        \draw[->] (Metre |- 0,-4) -- node[above,sloped,midway]{$b=1$} node[at end,right]{1} (Thread1 |- 0,-4.5);
        \draw[->] (Thread2 |- 0,-5) -- node[above,sloped,midway]{requestBar(0)} node[at start,right]{0} node[at end,left]{1} (Metre |- 0,-6);
        \draw[->] (Metre |- 0,-6) -- node[above,sloped,midway]{$b=1$} node[at end,right]{1} (Thread2 |- 0,-6.5);

    \end{tikzpicture}
    \caption{Sequence diagram demonstrating the behaviour of bar number synchronisation with multiple threads. Values along the vertical axes show that thread's bar number counter. Arrows represent method calls and their return values.}
    \label{fig:multi-threading_sequence_diagram}
\end{figure}
\pagebreak

Figure~\ref{fig:multi-threading_sequence_diagram} is a sequence diagram demonstrating the behaviour of the synchronisation
in the presence of multiple threads. Each object/thread's bar number counter is
shown along its axis. Each thread's counter is initialised to 0. Thread 1
requests bar 0, which is equal to the SynchronisedMetre's current value, so $b=0$
is returned. Thread 1 then requests the next bar number (1). This is greater
than the SynchronisedMetre's current value, so its value is updated and returned.
Thread 2 then starts late and requests an old bar number (0). The
SynchronisedMetre recognises this is old, so simply returns its current value
$b=1$ so Thread 2 can catch up.



\section{Jembe data analysis} \label{jembe_data_analysis}

Creating music with realistic micro-timing using the implementation I have
described requires a set of probability distributions which accurately
characterise a style of music. Therefore, these distributions must be derived
from real-life examples of music. The next two sections describe the data
collection and analysis that was done for the jembe and Viennese waltz styles.
The results of the data analysis methods described here can be found in
Section~\ref{data_analysis_results}.


\subsection{Datasets} \label{jembe_datasets}

Previous research into the micro-timing of Malian jembe music has meant there
are some high-quality datasets of processed live recordings available.

The first dataset is from Jacoby et al.\ \cite{jacoby2021} and consists of 11
processed recordings of a piece called `Suku', which is a very commonly played
piece in this style. The second dataset is from the Interpersonal Entrainment in Music Performance
(IEMP) Data Collection \cite{polak2020,clayton2021}. This consists of 15 recordings
across three different pieces: `Manjanin', `Maraka', and `Woloso'. Both datasets here use recordings made by Rainer Polak in Mali -- see Appendix~\ref{appendix_datasets_jembe} for more recording details and a list of performers.

The datasets supply the following data:
\begin{itemize}
	\item Onset time -- the time when a drum stroke occurs
	\item Cycle number
	\item Metric location -- which metrical event the onset belongs to
	\item Phase -- the actual timing of the onset within the cycle
\end{itemize}


\subsection{Micro-timing estimation} \label{jembe_micro-timing}

The pieces of jembe music in the dataset use a metre\footnote{See Polak \cite{polak2010} for an ethnographically sensitive discussion of the extent to which metre applies in this context.} with four beats, each of
which divides into three, for a total of 12 metrical events at the first division
level (\setmetre{12}{8} in Western classical notation). It is at this level, referred to as the `pulse', that the micro-timing occurs.

Each event at the pulse level has a position in the cycle at which a note would
occur if it were isochronous (the \emph{categorical} timing). The displacement from this position describes the
micro-timing (or \emph{expressive} timing) of the note, and this is what is stored in the probability
distributions described in Section~\ref{style_class}. Equation~\ref{eq:displacement_from_phase} shows how the displacement is calculated from the phase given by the datasets.
\begin{equation}
    \textnormal{displacement} = \frac{(\textnormal{phase} \times \textnormal{beat division}) - \textnormal{metric location}}{2}
    \label{eq:displacement_from_phase}
\end{equation}
The phase is multiplied by the beat division (in this case, 3) to convert it
into pulse units. The metric location at the pulse level (an integer from 0 to 11)
is subtracted to get the displacement. The final division by two converts the displacement
into quarter lengths (because each pulse unit is an eighth length).

For example, if an onset has metric location $=6$ and phase $=2.01$, the displacement would be calculated by:
\[\textnormal{displacement} = \frac{(2.01 \times 3) - 6}{2} = \frac{0.03}{2} = 0.015 \textnormal{ quarter lengths}\]

Once the displacement has been calculated for each drum stroke, I was then able to
estimate the distribution of displacements for each of the twelve metric locations. I
considered two types of approach for this probability density estimation:

\begin{itemize}
	\item Kernel density estimation (KDE) (non-parametric) -- applies smoothing to the
data to fit an arbitrary distribution.
	\item Maximum likelihood estimation (MLE) (parametric) -- estimates the parameters of a given existing distribution which best fit the data.
\end{itemize}

The advantage of KDE is that it would be able to accurately represent any shape
of distribution the data may have. However, each point in
the probability density function has to be stored individually. This means it generally has a higher memory
requirement than MLE, which only stores the values of the parameters. Another
advantage of MLE is it allows hypothetical distributions to be easily defined
without the need for data.

For this scenario, I chose to use MLE to fit a normal distribution to the data.
A normal distribution was suitable because the data was assumed to be estimating
a theoretical true value of the displacement. A normal distribution requires two
parameters: the mean, $\mu$, and the standard deviation, $\sigma$. It can be shown that the maximum likelihood estimator $\hat{\mu}$ for the population mean is the sample mean $\bar{x}$, and the estimator $\hat{\sigma}$ for the population standard deviation is the sample standard deviation $s$ \cite{dekking2005}. The calculation of $s$ uses Bessel's
correction to get an unbiased estimator of $\hat{\sigma}$ \cite{upton2014}.


\subsection{Tempo estimation} \label{tempo_estimation}

Generating a synthetic piece of jembe music requires analysis of other musical
features as well as the micro-timing to sound realistic. One of these is the
\emph{tempo} (how fast or slow the beat is), which is measured in beats per minute
(bpm). The tempo of a jembe piece of music typically increases substantially
over the duration of the performance \cite{jacoby2021}, with the last 15 seconds or
so showing the tempo increasing at a much faster rate.

The \emph{inter-beat interval} (IBI) is defined as the time between two consecutive beats in a
piece of music, from which the instantaneous tempo can be calculated \cite{dixon2001}.
A moving average can be applied to the instantaneous tempo to obtain an estimate
of the global tempo.

For the jembe data, I first filtered all the onsets to include just those played
by Jembe 2 (because it plays on every beat \cite{jacoby2021}), then filtered these to
just onsets on the beats. I then calculated the inter-beat interval using Equation~\ref{eq:ibi}, where $b_i$ is the onset time of the $i$th beat. This is then converted to bpm by Equation~\ref{eq:tempo}. A moving average with window size 10 is then applied to smooth the tempo estimate.
\begin{equation}
    \mathrm{IBI}_i = b_i - b_{i-1}
    \label{eq:ibi}
\end{equation}
\begin{equation}
    t_i = \frac{60}{\mathrm{IBI}_i}
    \label{eq:tempo}
\end{equation}
Inspection of the smoothed tempo graphs (Section~\ref{jembe_tempo_estimation_results}) showed a logarithmic trend for the first $\sim95\%$ of the piece. A sharper increase follows this which was
modelled by a quadratic curve. To fit curves to the data, I used the \verb'optimize.curve_fit' function from the SciPy Python library, which uses a non-linear least
squares method \cite{more1977}. The parameters estimated by the curve fitting are then used in Sonic Pi
to control the tempo of a synthetic jembe piece during playback.


\subsection{Rhythm patterns} \label{rhythm_patterns}

The final data analysis performed on the jembe music was an analysis of the
rhythm patterns played by Jembe 1. While the other instruments in the ensemble
play repetitive accompaniments, Jembe 1 has a lead role and can play a variety
of rhythmic patterns, also often involving improvisation. By analysing these
patterns, I was able to produce a synthetic Jembe 1 part for Suku with
semi-realistic rhythms. My approach was as follows:

\begin{enumerate}
    \item Compute the rhythm played by Jembe 1 in each cycle as a 12-bit binary number, where the bit value at each position indicates whether there was an onset at the corresponding pulse in that cycle.
    \item Convert the 12-bit binary numbers into a unique decimal integer for each cycle.
    \item Count the number of times rhythm $x$ is followed by rhythm $y$ and calculate transition probabilities from these.
    \item Generate a random sequence of rhythm integers using the transition probabilities.
    \item During playback, for each cycle, play a drum stroke on pulse $i$ if there is a 1 at bit $i$ in the binary representation of that cycle's rhythm integer.
\end{enumerate}

This approach was an improvement over purely random rhythms because it preserved
intra-cycle patterns. The jembe can be played with three different techniques,
each producing a different kind of sound (timbre). These are tone, slap, and
bass. To include this, I created a roughly typical pattern covering each note
position in the cycle: [T,T,S,T,T,S,B,B,S,B,B,S]. If a note is played at a
particular pulse, the corresponding timbre is looked up from this pattern.



\section{Waltz data analysis} \label{waltz_data_analysis}

The Viennese waltz provides a useful comparison to Malian jembe in evaluating
this project's micro-timing implementation. This style uses a metre with three
beats (\setmetre{3}{4}) and the micro-timing can be observed at the beat level, where the
second beat is usually early. The micro-timing in Viennese waltz has not been
studied in as much detail or as recently as jembe, so there were no existing
datasets of Viennese waltz performances with micro-timing. This meant further
calculations were needed to derive it, which are explained in this section.


\subsection{Datasets} \label{waltz_datasets}

I first considered using the Ballroom dataset \cite{gouyon2006}, which contains
30-second recordings of 65 pieces of Viennese waltz music, and for which
annotations of the beats exists \cite{krebs2013}. However, when I listened to the
recordings, I did not notice any micro-timing. To test my suspicions, I performed
the micro-timing analysis described in Section~\ref{waltz_micro-timing} and then conducted a
one-sample \textit{t}-test. This tests whether the sample mean of the displacement of the
second beat is statistically significantly different from 0 (the case where
there is no micro-timing). At the 5\% significance level, there were very few
recordings which had any significant micro-timing, so this dataset was deemed
unsuitable.

I then constructed my own dataset comprising of 30-second samples from seven
waltz recordings performed by the Vienna Philharmonic Orchestra, all of which
have noticeable and statistically significant micro-timing. See Appendix~\ref{appendix_datasets_waltz} for more details. Because this dataset
has no existing beat annotations, the next step was to produce my own.


\subsection{Beat tracking} \label{beat_tracking}

\emph{Beat tracking} is the process of identifying the locations of beats in an audio
recording of a piece of music. Since the beat level is where the micro-timing in
the Viennese waltz occurs, no additional onset detection was necessary. The beat
tracking could be approached automatically with existing beat tracking
algorithms, or manually.

A variety of automatic beat tracking algorithms were tried, but most did not
perform well. Many implementations struggled because they expect the beats to be
isochronous, i.e.\ having no systematic micro-timing, as is the case in most
Western music. As a result, they would often skew the detected beats towards
being isochronous, therefore not capturing the micro-timing. Of all these
implementations, the beat tracking in the libfmp Python library \cite{mueller2021} (a
dynamic programming approach introduced by M√ºller \cite{mueller2021b}) performed the best. After some
small manual corrections, the beat onsets were ready for analysis, as described
in Section~\ref{waltz_micro-timing}.

As an alternative, manual beat annotations were created by hand using the Sonic
Visualiser software \cite{cannam2010}. While the micro-timing from this approach was
more prominent, the data had a large variance (likely due to human imprecision) which led to the generated music sounding erratic. For this reason, the automatic approach was chosen.


\subsection{Micro-timing estimation} \label{waltz_micro-timing}

To calculate the micro-timing displacement of each beat from just the onset time
involves first identifying the start and end of each cycle, estimating the onset
of each beat as if they were isochronous, then finding the difference between this
and the actual onset to get the displacement. The full calculation is shown in
Equation~\ref{eq:displacement_from_onset}, where $i$ ranges over the indices of all the onsets in the piece.
\begin{equation}
\begin{split}
    \mathrm{metricLocation}_i &= i \;\mathrm{mod}\; 3 \\
    \mathrm{cycleStart}_i     &= \mathrm{onset}_{i-\mathrm{metricLocation}_i} \\
    \mathrm{cycleEnd}_i       &= \mathrm{onset}_{i-\mathrm{metricLocation}_i+3} \\
    \mathrm{cycleDuration}_i  &= \mathrm{cycleEnd}_i - \mathrm{cycleStart}_i \\
    \mathrm{isochronousBeatDuration}_i &= \frac{\mathrm{cycleDuration}_i}{3} \\
    \mathrm{isochronousOnset}_i &= \mathrm{cycleStart}_i + (\mathrm{metricLocation}_i \times \mathrm{isochronousBeatDuration}_i) \\
    \mathrm{displacement}_i &= \frac{\mathrm{onset}_i-\mathrm{isochronousOnset}_i}{\mathrm{isochronousBeatDuration}_i} \\
    \mathrm{phase}_i          &= \mathrm{displacement}_i + \mathrm{metricLocation}_i
\end{split}
\label{eq:displacement_from_onset}
\end{equation}
The data is assumed to have exactly one onset for each beat in the piece, and the
duration of a cycle is assumed to be the time between its first beat and the
first beat of the next cycle.

Once the displacements had been derived, maximum
likelihood estimation was used to fit the probability distributions, as described in
Section~\ref{jembe_micro-timing}.
\newpage




\chapter{Evaluation} \label{evaluation}

This chapter aims to demonstrate the capabilities of my project and prove the
achievement of the success criteria.

I begin by showing the results of the
data analysis which was conducted as part of the implementation. This is a
crucial step towards being able to produce music with realistic micro-timing
because it provides the data needed to populate the styles' probability
distributions. Secondly, I describe the motivation, methods, and results of
a user study I conducted to evaluate how realistic the generated music sounds.
This includes a detailed description of how the study was carried out, how the
stimuli were constructed, and the full results. The section concludes with a
discussion of the results and their significance. Finally, I use the MusicXML
converters implemented earlier to demonstrate the value gained by an
implementation of metre within Sonic Pi and showcase its successful use.



\section{Data analysis results} \label{data_analysis_results}

This section displays the results of the data analysis described in Sections~\ref{jembe_data_analysis} and~\ref{waltz_data_analysis}. I
begin by describing the data-derived micro-timing for jembe and Viennese waltz,
and the patterns which occur in them. Jembe is found to have a short-medium-long
pattern in its pulses, and the waltz has a significantly early second beat. I
then describe the results of the jembe tempo estimation.


\subsection{Micro-timing estimation} \label{micro-timing_estimation_results}

Figure~\ref{fig:suku_histogram} shows the results of the micro-timing estimation (Section~\ref{jembe_micro-timing}) for one of
the jembe pieces, `Suku'. The histograms show the positions within the cycle
where each of the 12 pulses occurred (phase). The dashed lines show where the
event would occur if they were isochronous, so the existence of the micro-timing
can be seen clearly by the positions of the second and third pulses in each beat.
By examining the positions of the histograms, we can see that the length of each pulse follows a short-medium-long pattern (SML), which is consistent across
each beat. Also shown are the probability density functions (PDF) of the
maximum-likelihood estimated normal distributions. The plots for
the other jembe pieces showed the same pattern.

\begin{figure}[ht]
    \centering
    \input{figures/suku_histogram.pgf}
    \caption{A histogram plot of the positions of each pulse within the cycle for Suku. The histograms are coloured by which beat they belong to. Dashed lines show the metrical grid. Black curves show the PDF of the MLE-derived probability distributions.}
    \label{fig:suku_histogram}
\end{figure}

Figure~\ref{fig:waltz_histogram} shows the results for the waltz dataset (Section~\ref{waltz_data_analysis}). The calculations
use the first beat as the definition for the start of the cycle, so every Beat 1
has a displacement of 0, which is why its histogram has no variance. The early onset
of the second beat can be clearly seen in the plot ($\mu=-0.0743$, $\sigma=0.0795$). A
one-sample \textit{t}-test confirms the micro-timing is significant ($t=-16.5$, $p=0.000$). Beats 1 and 3 show no significant deviation from the metrical grid, so a short-long-medium pattern (SLM) is observed.

\begin{figure}[ht]
    \centering
    \input{figures/waltz-histogram.pgf}
    \caption{A histogram plot of the displacement of each beat for the waltz dataset. Dashed lines show the metrical grid. Black curves show the PDF of the MLE-derived probability distributions.}
    \label{fig:waltz_histogram}
\end{figure}


\subsection{Jembe tempo estimation} \label{jembe_tempo_estimation_results}

Figure~\ref{fig:suku_tempo} shows the change in estimated tempo over the duration of the piece for
`Suku', as described in Section~\ref{tempo_estimation}. The tempo starts at around 135~bpm at the beginning of the piece and
ends at around 175~bpm. The dark blue line shows the smoothed mean tempo, and
the black lines show the logarithmic and quadratic curves fit to the data. The
shaded area shows $\pm1$ standard deviation from the mean.

The results show the increase in tempo throughout the piece that is characteristic of Malian jembe music. The more dramatic speedup at the end is also reflected in this data -- this is why I fit two different curves to the data. The tempo results match those found by Jacoby et al.\ \cite{jacoby2021supp}, and each jembe piece showed the same trend.

\begin{figure}[ht]
    \centering
    \input{figures/suku-tempo.pgf}
    \caption{Tempo estimation for `Suku'. Grey: Smoothed estimated tempo for each recording. Blue: Mean estimated tempo averaged over all recordings. Shaded: One standard deviation from the mean. Black: Curves fit to the data.}
    \label{fig:suku_tempo}
\end{figure}




\chapter{Conclusions} \label{conclusions}

In this project, I have investigated and implemented probabilistic
style-specific micro-timing in a musical live coding language. To do this, I
extended the Sonic Pi language with an implementation of musical metre and
micro-timing, and developed a method of synchronising this micro-timing between
multiple threads. I then performed data analysis on recordings of music from two
case study styles to generate music with realistic micro-timing.

This chapter summarises the key achievements of my project, the lessons learnt,
and some possibilities for future work in this area.



\section{Achievements} \label{achievements}

The primary aim of this project was to extend the Sonic Pi live coding language
to allow users to create music with style-specific micro-timing. This required
the music to be written in a metrical context, which Sonic Pi does not have. I
have successfully implemented a functional concept of musical metre, using my
MetreLeaf and MetreTree classes to construct arbitrary metrical hierarchies and
perform transformations on them (Section~\ref{metre_implementation}).

Having metre in Sonic Pi is also useful in its own right as it allows for the
representation and manipulation of metrical structures within a piece of music.
This utility, as well as the correct operation of my implementation, was
demonstrated successfully in the evaluation (Section~\ref{metre_evaluation}).

Probabilistic micro-timing has been implemented by using normal distributions to
define the displacement of each metrical event in a cycle from its position on the
grid. Random samples are drawn from these distributions during playback to
adjust the timing of notes (Section~\ref{micro-timing_implementation}).

The second key part of my success criteria was to be able to use this
implementation to generate music with realistic micro-timing. To do this, I
performed data analysis on recordings of music from the Malian jembe and
Viennese waltz styles (Sections~\ref{jembe_data_analysis} and~\ref{waltz_data_analysis}). This allowed me to derive probability distributions and
generate appropriate music. By including Viennese waltz, I was able to achieve
one of my extension criteria.

I then conducted a user study to evaluate the realism of the synthetic music (Section~\ref{user_study}).
The results of this were significant for the waltz style, which demonstrated my
project's success in achieving this. I also received feedback from an expert in
jembe music, who described the results as ``realistic and beautiful''.

In summary, the original success criteria were to: represent micro-timing probability distributions, implement style-specific micro-timing, and conduct a user study. The achievements I have described fulfil these criteria and one extension.



\section{Future work} \label{future_work}

There are a variety of possibilities for future work in this area. Firstly, a
useful extension to my project would be to add a GUI feature to Sonic Pi's IDE
which allows users to define their own micro-timing presets and interact with
the probability distributions directly.

An obvious extension would be to perform the data analysis for a number of
additional styles. Other styles with well-known micro-timing include jazz swing
rhythms \cite{dittmar2018}, candombe drum ensembles from Uruguay \cite{jure2016,fuentes2019}, and Brazilian samba music \cite{naveda2011,fuentes2019}. Furthermore,
creation of a larger dataset of Viennese waltz recordings would be beneficial in
deriving more accurate distributions.

Finally, future work could also involve implementing a control within Sonic Pi
to dynamically adjust the ``strength'' of the micro-timing, i.e.\ to transform the
timing displacements between being style-specific and isochronous.





\printbibliography[heading=bibintoc]





\appendix





\chapter{Datasets} \label{appendix_datasets}

This appendix details the datasets used in the data analysis (Sections~\ref{jembe_data_analysis} and~\ref{waltz_data_analysis}) and user study (Section~\ref{user_study}).



\section*{Jembe} \label{appendix_datasets_jembe}

Two datasets of recordings of jembe performances were used.

The first is from Jacoby et al.\ \cite{jacoby2021} and consists of 11 recordings of a piece called `Suku', which is very commonly played in Mali. The recordings were made by Rainer Polak in Bamako, Mali in 2016. They feature Drisa Kone on Jembe 1, Sedu Keita on Jembe 2, and Madu Jakite on Dundun \cite{jacoby2021supp}. The dataset can be accessed from \url{https://osf.io/8wyav/}.

The second dataset is part of the Interpersonal Entrainment in Music Performance (IEMP)
Data Collection \cite{polak2020,clayton2021} and consists of 15 recordings across three different pieces: `Manjanin', `Maraka', and `Woloso'. The recordings were made by Polak in Bamako in 2006/7. They feature performances by Drisa Kone, Sedu Balo, Madu Jakite, Antoine Traole, Isa Coulibaly, and Jeli Madi Kuyate. Each recording is between 2 and 6 minutes in length. The dataset can be accessed from \url{https://osf.io/m652x/}.

For the jembe section of the user study, participants were primed on four recordings of jembe music (Section~\ref{priming}). The first two of these are Audio 1a and 1b from Polak \cite{polak2010}. These are 14-second loops of Cycle 178 from his 2006 recording of Manjanin. Audio 1b isolates the Jembe 1 part from Audio 1a. The third example was a 13-second loop of Cycles 17 and 18 from the same recording. I included this so participants could be shown an example at a slower tempo. The final example was the full recording of Manjanin, which lasted 5 minutes.
\newpage



\section*{Waltz} \label{appendix_datasets_waltz}

Due to the lack of suitable existing datasets for Viennese waltz discussed in Section~\ref{waltz_datasets}, I created a new dataset consisting of 30-second samples from seven recordings of waltz pieces performed by the Vienna Philharmonic Orchestra. The full dataset is shown in Table~\ref{table:waltz_dataset}. All pieces were composed by Johann Strauss II, except for Recording 4, which was composed by Johann Strauss I. For the waltz section of the user study, participants were primed on Recordings 4, 5, and 7 from the dataset (Section~\ref{priming}). 

\begin{table}[ht]
    \begin{tabularx}{\linewidth}{
        l
        >{\raggedright\arraybackslash}X
        l
        l
    }
        \toprule
        ID & Title & Conductor & Recording date \\
        \midrule
        1 & The Blue Danube & Georges Pr√™tre & 1st January 2010 \\
        2 & ``Die Fledermaus'' Overture & Zubin Mehta & 29th May 1999 \\
        3 & Kaiser-Walzer & Herbert von Karajan & 1st January 1987 \\
        4 & Loreley-Rhein-Kl√§nge & Zubin Mehta & 29th May 1999 \\
        5 & Tales From The Vienna Woods & Daniel Barenboim & 1st January 2014 \\
        6 & Wiener Blut & Zubin Mehta & 29th May 1999 \\
        7 & Wiener Bonbons & Zubin Mehta & 29th May 1999 \\
        \bottomrule
    \end{tabularx}
    \caption{Table of recordings included in the Viennese waltz dataset}
    \label{table:waltz_dataset}
\end{table}





\end{document}
