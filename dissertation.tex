\documentclass[12pt,twoside,openright]{report}

% Hyperlink references
\usepackage[pdfborder={0 0 0}]{hyperref}
% PGF diagrams
\usepackage{pgf}
\usepackage[utf8]{inputenc}\DeclareUnicodeCharacter{2212}{-}
% Page margins
\usepackage[margin=25mm]{geometry}
% Subfigures
\usepackage{caption}
\usepackage{subcaption}
% Bibliography
\usepackage[style=numeric-comp,sorting=none]{biblatex}
% Images
\usepackage{graphicx}
% Directory tree diagram
\usepackage{dirtree}
% Simple tree diagrams
\usepackage[linguistics]{forest}
\usepackage{adjustbox}
% Algorithms
\usepackage[ruled]{algorithm2e}
% Better verbatim
\usepackage{fancyvrb}
% TikZ diagrams
\usepackage{tikz}
% UK date
\usepackage[UKenglish]{babel}
\usepackage{csquotes}
% Paragraph spacing
\usepackage{parskip}
% Verbatim in caption
\usepackage{cprotect}
% Aligning equations
\usepackage{amsmath}
% Auto-column tables
\usepackage{tabularx}
% Better table lines
\usepackage{booktabs}
% Multi-row and column tables
\usepackage{multirow}
% Code syntax highlighting
\usepackage{minted}
% Multi-column environments
\usepackage{multicol}


\addbibresource{bibliography.bib}

\raggedbottom

\DeclareRobustCommand{\setmetre}[2]{\ensuremath{
  \vcenter{\offinterlineskip
    \halign{\hfil##\hfil\cr
            $\scriptstyle#1$\cr
            \noalign{\vskip1pt}
            $\scriptstyle#2$\cr}
  }}\!
}

\usemintedstyle{sonicpi}

\begin{document}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Title

\begin{titlepage}
    \pagestyle{empty}

    \rightline{\LARGE \textbf{Max Johnson}}

    \vspace*{60mm}
    \begin{center}
    \Huge
    \textbf{Musical micro-timing for live coding} \\[5mm]
    Computer Science Tripos -- Part II \\[5mm]
    Clare College \\[5mm]
    \today
    \end{center}
\end{titlepage}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Proforma, table of contents and list of figures

\pagestyle{plain}

\section*{Declaration}

I, Max Johnson of Clare College, being a candidate for Part II of the Computer Science Tripos, hereby declare that this dissertation and the work described in it are my own work, unaided except as may be specified below, and that the dissertation does not contain material that has already been used to any substantial extent for a comparable purpose. I am content for my dissertation to be made available to the students and staff of the University.

\bigskip
\leftline{\textbf{Signed:} Max Johnson}

\medskip
\leftline{\textbf{Date:} \today}


\newpage
\chapter*{Proforma}

{\large
\begin{tabularx}{\linewidth}{lX}
Candidate number:   & \bf 2388B \\
Project title:      & \bf Musical micro-timing for live coding \\
Examination:        & \bf Computer Science Tripos -- Part II, July 2022  \\
Word count:         & \bf TBC \\
Code line count:    & \bf 1,935\footnotemark[1] \\
Project originator: & \bf Mark Gotham \\
Supervisor:         & \bf Mark Gotham \\
\end{tabularx}
}
\footnotetext[1]{Computed with \texttt{cloc}: \url{https://github.com/AlDanial/cloc}}
\stepcounter{footnote}


\section*{Original aims of the project}

TBC

\section*{Work completed}

TBC

\section*{Special difficulties}

TBC
 
\newpage
\tableofcontents

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\pagestyle{headings}





\chapter{Introduction} \label{introduction}

In this project, I present a novel system for generating music with
style-specific micro-timing in the Sonic Pi live coding language. The aim was to
use a probabilistic approach to control the exact timing of music written in
Sonic Pi according to realistic deviations found in music across the world. My
implementation introduces the concept of musical metre into Sonic Pi, for which
I demonstrate its successful use and then utilise to create micro-timing. I also
perform data analysis on the micro-timing of two different styles of music and
evaluate the realism of the resulting music. This achieves the key success
criteria set out in my project proposal and one suggested extension.



\section{Motivation} \label{motivation}

Micro-timing in music refers to small deviations in the timing of notes from
their exact, grid-like positions in the metre (see Section~\ref{metre_background} for details). These
deviations occur naturally in all human performances, but when they occur
systematically, they make an important contribution to what defines a musical
style. Therefore, this a very valuable feature when creating realistic music.
Many commonly used music synthesis environments (such as Sonic Pi) lack the
ability to control micro-timing during playback beyond simple jazz swing effects.
By implementing this, I aim to give Sonic Pi composers the ability to produce
music in certain styles more realistically. It also allows for further
creativity, such as by applying the micro-timing strategies of one style to
music from another style.

Music live coding (Section~\ref{live_coding_background}) is a way of creating and performing music by writing
and modifying code in real time. This was an interesting area to focus on for
this project because the nature of live coding allows for creative applications
of micro-timing, such as by adjusting the micro-timing during a performance.

The second major component of my project was the data analysis which I performed
on recordings of music from two case study styles. The motivation for this was
to be able to use my implementation to generate music from these styles which
has realistic micro-timing.



\section{Related work} \label{related_work}

Creating music with synthetic micro-timing has been explored as part of music
information retrieval (MIR) for a long time. This is often attempting to model
human-like expressive timing \cite{bilmes1993}. Oore et al.\ \cite{oore2020} used an
LSTM-based recurrent neural network to generate piano music with expressive
timing and dynamics. Huang and Yang \cite{huang2020} use a transformer model with
metrical constraints to automatically compose pop music. The FlexGroove\footnote{\url{https://www.ableton.com/en/packs/flexgroove/}} tool for Max For Live takes a different approach, and gives the user
control over swing and other micro-timing deviations, including a probabilistic
element.

Most live coding languages lack a full hierarchical representation of musical
metre (Section~\ref{metre_background}), typically only featuring a concept of beats and time signatures.
For example, the Max/MSP language uses its transport object to allow
access to bar and beat numbers for the current time signature. Gibber
\cite{roberts2012} emphasises a \setmetre{4}{4} time signature by default, and allows users to
trigger music to play at the beginning of a bar. An exception is McLean's Tidal
Cycles \cite{mclean2010} which uses a cyclic notion of time which can be subdivided
to achieve more complex hierarchies. Tidal also has a \verb'swingBy' function\footnote{\url{https://tidalcycles.org/docs/patternlib/tour/time/}}, which
implements a simple jazz swing micro-timing effect.

In summary, though there has been much research into the analysis of
micro-timing in different musical styles, and the application of expressive
timing to computer-generated music, implementations of style-specific
micro-timing as part of music software are rare. Additionally, musical metre is
a concept often underrepresented in live coding. With this project, I hope to
address this with my implementation for Sonic Pi.





\chapter{Preparation} \label{preparation}

This chapter will introduce the core theories and background material necessary
to understand the project. I will explain some key concepts in music theory and
live coding and give an overview of the case study musical styles. Next, I will
describe the starting point of this project, expand upon the requirements from
the project proposal, and conclude with an outline of the software engineering
techniques used throughout.



\section{Background} \label{background}


\subsection{Musical metre} \label{metre_background}

A beat is a regularly occurring pulse in music. This is usually the pulse you
might tap your foot along to when listening to a piece of music. Musical metre
describes the ways in which beats are grouped and divided within recurring
cycles. This forms different metrical levels within a hierarchy. Division levels
are ones where the beat has been divided into more, shorter durations. Multiple
levels are ones where the beats are grouped into fewer, longer durations. Figure~\ref{fig:metre_hierarchy_example} 
shows the beat level, and some of the division and multiple levels for a simple \setmetre{4}{4} time signature, as well as a tree representation of the metrical hierarchy.
A metre is `simple' if its first division level splits the beat into two, and
`compound' if it splits into three \cite{omt2021}.

\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{figures/metre_example.pdf}
    \caption{Example metrical hierarchy for the \setmetre{4}{4} time signature. Left: The hierarchy as notes in traditional Western music notation. Right: The hierarchy as a tree of durations.}
    \label{fig:metre_hierarchy_example}
\end{figure}

Every note in a piece of music occurs at a particular metrical position, but
music performed by humans often deviates from these mechanically even timings
\cite{london2012}. The ways in which these deviations occur contribute significantly
to what we call musical style by producing micro-timing effects. Different
musical styles have different micro-timing strategies. A common example is the
long-short pattern in swing rhythms, where every other eighth note (the first
division level) is slightly delayed. This timing information is typically lost
in music notation, so musicians have to use
their knowledge and experience of the style to inform their performance of the
micro-timing. This is formalised in Justin London's Many Meters Hypothesis
\cite{london2012}. A metre without any micro-timing is said to be `isochronous',
which means the metric events at each level are equally spaced in time.


\subsection{Live coding} \label{live_coding_background}

Music live coding is a way of creating and performing music by writing and
modifying code in real time. The performer typically projects their code onto a
screen for an audience to follow along with \cite{magnusson2011}. The liveness of the
performance is important -- it is argued that the musician must directly
interact with the running algorithms for it to be truly considered live coding
\cite{collins2011}. Otherwise, there is little difference between this and playing a
recording of synthesised music.

Sonic Pi is a popular live coding language and IDE created by Sam Aaron, which
was designed as an educational tool for teaching programming in schools. It
implements its own domain-specific language written in Ruby, using the
SuperCollider sound synthesis server to produce sounds \cite{aaron2013}. A simple
Sonic Pi program might look like the following:

\begin{minted}{ruby}
play :C4
sleep(1)
play :C5
sleep(0.5)
play :C4
\end{minted}

This plays the note C4 on the current synthesiser, sleeps for one beat, plays C5,
sleeps for half a beat, then plays C4 again.

A key component of the Sonic Pi language is the \verb'live_loop', which executes a
block of code in a loop in a new thread. This multi-threading allows multiple instruments/parts to be played together. To maintain correct timing in the presence
of multiple threads and long execution time, Sonic Pi uses sophisticated
``virtual time'' functionality behind-the-scenes \cite{aaron2014}.

One drawback of Sonic Pi is that it currently doesn't have a built-in notion of
metre or style-specific micro-timing, which is what this project addresses.


\subsection{Case studies} \label{case_studies}

This project uses two different styles of music as case studies for evaluation,
both of which have well-known micro-timing characteristics. The first is jembe
drum music from Mali, which has highly regular micro-timing, and the second is
Viennese waltz music, which offers a Western comparison.

Jembe ensemble music is a style of music involving a small group of drummers
which originated in West Africa. The primary drum is the eponymous jembe, which
is a goblet-shaped drum played with the bare hands. This is usually accompanied
by the dundun, which is a cylindrical drum played with a stick \cite{polak2010}. In a
traditional jembe trio, there are two jembe players and one dundun player, with
Jembe 1 having a lead role, Jembe 2 playing a simple accompaniment, and Dundun
playing a varying pattern that is characteristic of each piece of music
\cite{jacoby2021}. Different playing techniques produce various kinds of sound
(timbre) on each drum, which gives rise to the melodic qualities of the music
\cite{polak2010}.

Jembe music is an ideal case study for this project because it has a highly
consistent micro-timing strategy \cite{polak2010}, and Malian drummers have been shown to
have one of the lowest levels of timing variability between performers in the
world \cite{clayton2020}. Compared with other styles of music, jembe music is
relatively constrained in terms of its pitch, timbre, and number of instruments,
which allows for a clear focus on timing.

The Viennese waltz is a style of music intended for ballroom dancing. It is a
fast waltz in \setmetre{3}{4} at around 180 beats per minute (bpm) and is performed by a
classical Western orchestra. It has a characteristic short-long-long
micro-timing pattern for the length of its beats \cite{bengtsson1974,bengtsson1977}.



\section{Starting point} \label{starting_point}

The main implementation of this project is built on top of Sonic Pi, which
handles audio synthesis, sample playback, timekeeping, basic multi-threading,
and all lower-level software components. It includes a simple representation of
beats (although this acts more as a virtual clock than beats in a metrical sense)
and tempo but has no support for metre.

Otherwise:
\begin{itemize}
	\item No work towards this project had been started in advance
	\item Data analysis of recordings of jembe music used existing datasets
	\item I had previous experience of using the Python programming language
	\item Common software libraries such as pandas and Matplotlib were used
	\item I had previous knowledge of basic music theory concepts
\end{itemize}



\section{Requirements analysis} \label{requirements_analysis}

The project proposal (Appendix~\ref{project_proposal}) features a list of success criteria outlining the
key milestones that needed to be achieved for the project to succeed. It also
has a description of possible extensions. During the research and investigation
stage of the project, I refined these into the list of requirements shown in
Table~\ref{table:requirements}.

\begin{table}[ht]
\centering
\begin{tabularx}{400pt}{Xc}
    \toprule
    \textbf{Requirement}                                      & \textbf{Priority} \\
    \midrule
    Research and implement representation of musical metre              & High \\
    Implement commands to play music in a given metre                   & High \\
    Implement representation of probability distributions for a style   & High \\
    Use distributions to implement probabilistic micro-timing           & High \\
    Perform data analysis on jembe recording dataset                    & High \\
    Perform data analysis on Viennese waltz recordings                  & Medium \\
    Generate synthetic music for the jembe case study                   & High \\
    Conduct user study to evaluate realism of generated music           & High \\
    Implement converter to and from MusicXML                            & Low \\
    \bottomrule
\end{tabularx}
\caption{Refined list of requirements (and their priorities) that should be achieved for the project to succeed as planned}
\label{table:requirements}
\end{table}


\subsection{Interaction with Sonic Pi}

As part of the requirements analysis, I investigated different methods of
interaction between my code and the existing Sonic Pi software. It was important
to do this before starting the implementation to ensure this went ahead smoothly
and without trial-and-error.

One approach I considered was to intercept Open Sound Control (OSC) network
messages between Sonic Pi and SuperCollider and adjust the timing of the event
contained in the message. An advantage of this would be that my code would be relatively
unrestricted in which language it was written in, which would avoid needing to
spend time learning a new language. A small amount of Sonic Pi code would still
need to be modified, however, to redirect its OSC messages.

The alternative was to build my code into the Sonic Pi codebase. This would
require me to use Ruby, however this approach works at a higher level so Sonic Pi can
handle the lower-level details like the OSC protocol. The other advantages of this approach are
that it allows the use of existing musical concepts in Sonic Pi, such as beat and
tempo, and makes it easy to extend the Sonic Pi language with new commands.
These advantages led me to choose this approach for my implementation.



\section{Software engineering techniques} \label{software_engineering_techniques}

\subsection{Languages and tools} \label{languages_and_tools}

The main implementation of this project was written in the Ruby programming
language because it was an extension of Sonic Pi which is written in Ruby. The
data analysis was done in Python due to its excellent support for data science
and visualisation. The popular pandas, Matplotlib, NumPy, and SciPy Python
libraries were used for this. For automatic beat tracking, I used the librosa
and libfmp Python libraries, and Sonic Visualiser for manual corrections.
MusicXML files were created with the MuseScore music notation software and
processed via the \verb'music21' Python library.

Development of the project was done using the Visual Studio Code editor because
it has good support for different languages, debugging, and version control.
However, Sonic Pi code written for testing and evaluation was done in the
dedicated Sonic Pi IDE to take advantage of its playback, code completion, and
language reference features.


\subsection{Version control and backups}

To reduce the risk of data loss throughout this project, various version control
and backup systems were used. The Git version control system was used to manage
the code in this project, with remote repositories on GitHub acting as backup
storage. Windows' FileHistory also performed regular local backups, and I used
Google Drive as an additional external backup destination for project files not
managed by Git.


\subsection{Testing}

Due to the musical nature of this project, most of the testing was done by
evaluating some simple program in Sonic Pi and listening to the output.
Additionally, I wrote a series of unit tests using Sonic Pi's built-in testing
framework. These tests ensured the metre implementation functioned correctly for
metres with both simple and complex hierarchies.





\chapter{Implementation} \label{implementation}

The implementation of this project consists of a few main components.

The first is the addition of the micro-timing functionality into Sonic Pi.
This consists of an implementation of musical metre, new commands to play music
within a metrical context, the style-specific micro-timing itself, and a method
of multi-threaded synchronisation. The second main component is the data
analysis which is used to generate realistic micro-timing for musical styles.
This is done primarily for Malian jembe, but also for Viennese waltz in
preparation for the evaluation. The final component is a pair of converters
between the MusicXML file format and my extended Sonic Pi language, also
implemented for use in the evaluation.

This chapter details the data structures, algorithms, and approaches used to
implement these features.



\section{Metre} \label{metre_implementation}

Implementing a concept of musical metre within Sonic Pi was a crucial step
towards adding micro-timing functionality. Without it, the system has no way of
knowing where each note falls within the metrical cycle, and therefore no way of
knowing what timing adjustment it should apply. This section describes the way
the metrical hierarchy has been implemented as a tree data structure, and the
main algorithms which act on it. I then go on to describe the Bar class as a
representation of a single metrical cycle.


\subsection{Initial approach} \label{metre_initial_approach}

The first approach was to use a simple list of integers to represent a metre,
where each integer corresponds to a beat, and the value of each integer is the
number of pulses that beat is divided into (as used by Gotham \cite{gotham2015}). For example, [3,3,2,2] represents a metre where the first two
beats are subdivided into three, and the second two beats are subdivided into
two. For lower metrical levels (two or more subdivisions of the beat level), it
was assumed that they divide into two.

This form had the advantage of being a simple and compact representation which
handled common time signatures (such as \setmetre{4}{4} or \setmetre{6}{8}) well. However, this approach
does not generalise well to all possible metres, so I discarded this in favour
of a tree structure, as this captures the full hierarchy more easily.


\subsection{Metrical hierarchy as trees} \label{metrical_hierarchy}

A tree data structure was chosen as a more descriptive representation for the
hierarchy information within a metre. This is a common way to depict metre;
Forth provides a detailed mathematical treatment of trees used in this context
\cite{forth2012}. To ensure an implementation that works well as part of a
programming language, I used the popular \verb'music21' Python library as a basis for
designing my representation \cite{ariza2010}.

The tree structure is implemented by the MetreLeaf and MetreTree classes (see
Sections~\ref{metreleaf} and~\ref{metretree} for more details). Figure~\ref{fig:tree_object_hierarchy} shows the default nesting of these objects
for a \setmetre{4}{4} time signature, and each object's duration. Note the duration of a
parent node is the sum of the durations of its children.

\begin{figure}[ht]
    \centering
    \resizebox{\linewidth}{!}{
        \begin{forest}
            for tree=draw,
            [{\small \bfseries Metre \\ $d=1$}
                [{\small \bfseries MetreTree \\ $d=\frac{1}{4}$}
                    [{\small \bfseries MetreLeaf \\ $d=\frac{1}{8}$}]
                    [{\small \bfseries MetreLeaf \\ $d=\frac{1}{8}$}]
                ]
                [{\small \bfseries MetreTree \\ $d=\frac{1}{4}$}
                    [{\small \bfseries MetreLeaf \\ $d=\frac{1}{8}$}]
                    [{\small \bfseries MetreLeaf \\ $d=\frac{1}{8}$}]
                ]
                [{\small \bfseries MetreTree \\ $d=\frac{1}{4}$}
                    [{\small \bfseries MetreLeaf \\ $d=\frac{1}{8}$}]
                    [{\small \bfseries MetreLeaf \\ $d=\frac{1}{8}$}]
                ]
                [{\small \bfseries MetreTree \\ $d=\frac{1}{4}$}
                    [{\small \bfseries MetreLeaf \\ $d=\frac{1}{8}$}]
                    [{\small \bfseries MetreLeaf \\ $d=\frac{1}{8}$}]
                ]
            ]
        \end{forest}
    }
    \caption{An example of how MetreTree and MetreLeaf objects are nested to construct a metrical hierarchy for \setmetre{4}{4}. The total duration $d$ of each node is also displayed, and the duration of a parent node is the sum of the durations of its children.}
    \label{fig:tree_object_hierarchy}
\end{figure}


\subsection{MetreLeaf class} \label{metreleaf}

A MetreLeaf object is the leaf node of the metrical tree structure. It has an
instance variable fraction which is a Rational (a Ruby object for storing a
rational number as a simplified fraction) which represents the duration of the
MetreLeaf in the metre. This is expressed as a fraction of a whole note. For
example, a leaf node with the duration of a quarter note (also known as a
crotchet) will have the value $\frac{1}{4}$.

The class contains a \verb'subdivide()' method, which divides the MetreLeaf by two a
given number of times, $s$. It returns a new MetreTree with $2^s$ MetreLeaf children, each of value $f/2^s$ where $f$ is the fraction of the original MetreLeaf.


\subsection{MetreTree class} \label{metretree}

A MetreTree object represents the hierarchical tree or subtree of a metre. The
instance variable \verb'sequence' is an ordered list containing any combination of
MetreLeaf and other MetreTree objects representing this node's children. For
example, the previous example hierarchy in Figure~\ref{fig:tree_object_hierarchy} could also be written in list form as:
\[\left[\left[\frac{1}{8},\frac{1}{8}\right],\left[\frac{1}{8},\frac{1}{8}\right],\left[\frac{1}{8},\frac{1}{8}\right],\left[\frac{1}{8},\frac{1}{8}\right]\right]\]
Each list is a
MetreTree, and each fraction is a MetreLeaf. The MetreTree class contains
several methods for manipulating and extracting information from the metrical
hierarchy it represents. The most important two of these are explained in more
detail below.

\subsubsection{Getting metrical levels} \label{get_level}

The first commonly used method is \verb'get_level()', which returns a flat MetreTree at
a given metrical level $l$. A flat MetreTree is defined as one whose children are
only MetreLeafs, meaning there is no hierarchy (e.g.\ $\left[\frac{1}{8},\frac{1}{8},\frac{1}{8},\frac{1}{8},\frac{1}{8},\frac{1}{8},\frac{1}{8},\frac{1}{8}\right]$).

This method is split into two algorithms:
\begin{itemize}
    \item \verb'get_division_level()' computes the sequence for division levels ($l>0$), and the beat level ($l=0$)
	\item \verb'get_multiple_level()' estimates a possible sequence for multiple levels ($l<0$)
\end{itemize}

Algorithm~\ref{alg:getDivisionLevel} shows the \verb'get_division_level()' algorithm. For each child in the
sequence list, if it is a MetreTree, the method is recursively called until the
base case of $l=0$ is reached. At this point, all the children of that node are
combined into one MetreLeaf equal to the sum of their durations. If the child is
instead a MetreLeaf, it is subdivided $l$ times to reach the desired metrical
level.

\begin{algorithm}[H]
    \SetKw{KwForIn}{in}
    \SetKwData{Child}{child}
    \SetKwData{Sequence}{@sequence}
    \SetKwData{NewSequence}{newSequence}
    \SetKwData{Result}{result}
    \SetKwFunction{MetreTree}{MetreTree}
    \SetKwFunction{MetreLeaf}{MetreLeaf}
    \SetKwFunction{GetDivLevel}{getDivisionLevel}

    \caption{getDivisionLevel()}
    \KwIn{Target metrical level, $l$}
    \KwOut{New flat \MetreTree at level $l$}
    \BlankLine

    \NewSequence $\gets$ empty list\;
    \ForEach{\Child \KwForIn \Sequence}{
        \eIf{\Child is a \MetreLeaf}{
            \eIf{$l>0$}{
                append \Child subdivided $l$ times to \NewSequence\;
            }{
                append \Child to \NewSequence\;
            }
        }{
            \tcp{\Child is a \MetreTree}
            \eIf{$l>0$}{
                $r \gets$ recursive call to \GetDivLevel{$l-1$} method on \Child\;
                append $r$ to \NewSequence\;
            }{
                \tcp{Base of recursion}
                $m \gets$ combine all children of \Child into one \MetreLeaf\;
                append $m$ to \NewSequence\;
            }
        }
    }
    \Return{new \MetreTree made from \NewSequence}
    \label{alg:getDivisionLevel}
\end{algorithm}

The \verb'get_multiple_level()' algorithm performs an estimate of the structure of
higher metrical levels by clustering nodes together. It is an estimate because this information is not in the
MetreTree's representation of the metre. The algorithm recursively clusters nodes until
the desired metrical level $l$ is reached. The number of nodes combined in each cluster is
determined by the smallest prime factor of the number of nodes at the level below. For
example, if level $l+1$ has four nodes, they will be clustered in groups of two. If it
has nine nodes, they will be clustered in groups of three.

An important consideration when implementing the \verb'get_level()' method was to
maximise its efficiency, because it is called at least once per note. The
running time of the algorithm is proportional to the number of MetreLeafs in the
hierarchy, so to improve this I implemented a cache of metrical levels for each
MetreTree object to store the expensive computations for reuse.

Some examples of the output of \verb'get_level()' for the following hierarchy are shown in Table~\ref{table:get_level}:
\[
    \left[
        \left[\frac{1}{8},\frac{1}{8}\right],
        \left[
            \frac{1}{8},
            \left[\frac{1}{16},\frac{1}{16}\right]
        \right],
        \frac{1}{8},
        \frac{3}{4}
    \right]
\]

\begin{table}[ht]
    \centering
    \renewcommand{\arraystretch}{2.0}
    \begin{tabular}{|c|c|}
        \hline
        $l$     & \verb'get_level'$(l)$ \\
        \hline
        $2$     & $\displaystyle \left[ \frac{1}{16},\frac{1}{16},\frac{1}{16},\frac{1}{16},\frac{1}{16},\frac{1}{16},\frac{1}{16},\frac{1}{16},\frac{1}{32},\frac{1}{32},\frac{1}{32},\frac{1}{32},\frac{3}{16},\frac{3}{16},\frac{3}{16},\frac{3}{16} \right]$ \\
        $1$     & $\displaystyle \left[ \frac{1}{8},\frac{1}{8},\frac{1}{8},\frac{1}{8},\frac{1}{16},\frac{1}{16},\frac{3}{8},\frac{3}{8} \right]$ \\
        $0$     & $\displaystyle \left[ \frac{1}{4},\frac{1}{4},\frac{1}{8},\frac{3}{4} \right]$ \\
        $-1$    & $\displaystyle \left[ \frac{1}{2},\frac{7}{8} \right]$ \\
        $-2$    & $\displaystyle \left[ \frac{11}{8} \right]$ \\ [1ex]
        \hline
    \end{tabular}
    \renewcommand{\arraystretch}{1.0}
    \cprotect\caption{Examples of the output of \verb'get_level'$(l)$ at different metrical levels $l$ for the hierarchy $[[1/8,1/8],[1/8,[1/16,1/16]],1/8,3/4]$. Note how level $l=-1$ is formed by the clustering of level $l=0$.}
    \label{table:get_level}
\end{table}

\subsubsection{Getting exact metrical events} \label{metrical_level_indices}

The second key method in the MetreTree class is \verb'metrical_level_indices()'. For a
given offset into the metric cycle, this algorithm finds any metrical events
which the offset occurs exactly on, and returns their index. For the example shown in Figure~\ref{fig:metrical_level_indices_example}, offset $x$
occurs on the first event of all three levels, so the function would return $[0 \Rightarrow 0,1 \Rightarrow 0,2 \Rightarrow 0]$ (a Hash from level to index). Offset $y$ occurs only on the last event of
level 2, so the function would return $[2 \Rightarrow 7]$. This method is important because it
is used later to determine which micro-timing probability distributions should
be applied to a note at a given offset (see Section~\ref{applying_micro-timing}).

\begin{figure}[ht]
    \centering
    \resizebox{\linewidth}{!}{
        \begin{adjustbox}{valign=t}
            \begin{forest}
                for tree={no edge},
                [, [Level 0 [Level 1 [Level 2]]]]
            \end{forest}
        \end{adjustbox}\qquad
        \begin{adjustbox}{valign=t}
            \begin{forest}
                for tree={calign=first},
                [,phantom,name=Phantom1
                    [{$\frac{1}{4}$},name=First4
                        [{$\frac{1}{8}$}
                            [{$\frac{1}{16}$},name=First16]
                            [{$\frac{1}{16}$}]
                        ]
                        [{$\frac{1}{8}$}
                            [{$\frac{1}{16}$}]
                            [{$\frac{1}{16}$}]
                        ]
                    ]
                ]
                \node(xNode)[red] at (First16 |- Phantom1) {$x$};
                \draw[->,red] (xNode) to (First4);
            \end{forest}
        \end{adjustbox}\qquad
        \begin{adjustbox}{valign=t}
            \begin{forest}
                for tree={calign=first},
                [,phantom,name=Phantom2
                    [{$\frac{1}{4}$}
                        [{$\frac{1}{8}$}
                            [{$\frac{1}{16}$}]
                            [{$\frac{1}{16}$}]
                        ]
                        [{$\frac{1}{8}$}
                            [{$\frac{1}{16}$}]
                            [{$\frac{1}{16}$},name=Last16]
                        ]
                    ]
                ]
                \node(yNode)[red] at (Last16 |- Phantom2) {$y$};
                \draw[->,red] (yNode) to (Last16);
            \end{forest}
        \end{adjustbox}\qquad
    }
    \caption{An example metrical hierarchy for \setmetre{2}{4} showing which metrical events at each level (if any) offsets $x$ and $y$ occur on. $x$ occurs on the first event in all three levels. $y$ only occurs exactly on an event in Level 2, namely its last event.}
    \label{fig:metrical_level_indices_example}
\end{figure}


\subsection{Metre class} \label{metre_class}

The Metre class is a subclass of MetreTree which acts as a wrapper for the
hierarchy stored in a MetreTree. It implements functionality allowing a user to
specify a metre by a time signature string (e.g.\ \mintinline{ruby}{'4/4'}) for common metres, or a
nested list of Rationals representing the hierarchy. It also has a method which
converts a duration in quarter lengths (the length of one quarter note) to Sonic
Pi beats.


\subsection{Bar class} \label{bar_class}

A bar (or measure) is a common term used in Western music theory for a single
metric cycle. The Bar class is a representation of this, and each instance of it
has an associated metre. A Bar object is responsible for:
\begin{itemize}
	\item Keeping track of the playback position through the cycle
	\item Converting a note duration given as a metrical level and a count into
quarter lengths
	\item Checking if a note or rest fits in the remaining time in the cycle, and
updating the bar's playback position accordingly
\end{itemize}

A note duration is specified as a metrical level and a duration. The duration
is in units of an event at the specified metrical level, so acts as a multiplier.
For example, a note with level 0 and duration 3 will last for 3 beats. The
\verb'add_note()' method handles checking if a note fits into the bar, as shown in
Algorithm~\ref{alg:add_note}. If a note can't fit into the bar's remaining time,
an exception is raised. This ensures the bar obeys the duration of its metre.

\begin{algorithm}
    \SetKwData{NewOffset}{newOffset}
    \SetKwData{CurrOffset}{currentOffset}
    \SetKwFor{RepeatTimes}{repeat}{times do}{end}
    \SetKwFunction{NoteException}{CannotFitNoteException}
    \SetKw{Raise}{raise}

    \caption{addNote()}
    \KwIn{Metrical level, $l$}
    \KwIn{Duration, $d$}
    \KwIn{Total metre duration (in quarter lengths), $q$}
    \BlankLine

    $\NewOffset \gets \CurrOffset$\;
    $M \gets$ metre at level $l$\;
    \RepeatTimes{$d$}{
        \eIf{$\NewOffset \geq q$}{
            \Raise \NoteException\;
        }{
            $n \gets$ length of active event in $M$ at \NewOffset\;
            $\NewOffset \gets \NewOffset + n$\;
        }
    }
    $\CurrOffset \gets \NewOffset$\;
    \label{alg:add_note}
\end{algorithm}



\section{Playing music} \label{playing_music}

Now that the framework of musical metre has been established, we can look at how
this is used by the user to play music. I will describe the new Sonic Pi
commands I've implemented for controlling metre, and adding notes and rests.
Figure~\ref{fig:sonicpi_language_comparison} shows a comparison between the original Sonic Pi commands (left), my
alternative commands (centre), and traditional Western music notation (right)
for a single bar of \setmetre{4}{4}.

\begin{figure}[ht]
    \centering
    \begin{subfigure}[b]{0.2\textwidth}
        \centering
        \begin{minted}{ruby}
play :C4
sleep(1)
play :E4
sleep(1)
play :G4
sleep(0.5)
play :E4
sleep(0.5)
play :C4
        \end{minted}
        \caption{Old}
    \end{subfigure}
    %
    \begin{subfigure}[b]{0.4\textwidth}
        \centering
        \begin{minted}{ruby}
use_metre '4/4'

bar do
    add_note :C4, 0, 1
    add_note :E4, 0, 1
    add_note :G4, 1, 1
    add_note :E4, 1, 1
    add_note :C4, 0, 1
end
        \end{minted}
        \caption{New}
    \end{subfigure}
    %
    \begin{subfigure}[b]{0.3\textwidth}
        \centering
        \includegraphics[width=\linewidth]{figures/sonic_pi_comparison.pdf}
        \caption{Notation}
    \end{subfigure}
    \cprotect\caption{A single bar of music represented by the original Sonic Pi syntax, my new metre commands, and traditional Western music notation. Note how the original Sonic Pi syntax loses information about the metre. The second and third arguments to \verb'add_note' are the metrical level and duration.}
    \label{fig:sonicpi_language_comparison}
\end{figure}


\subsection{Metre commands} \label{metre_commands}

A metre is created with the \verb'use_metre' or \verb'with_metre' commands. These have been
designed to match the syntax of existing Sonic Pi commands such as \verb'use_fx' and \verb'with_fx'.

\begin{itemize}
	\item \verb'use_metre('$m$\verb')' changes the current thread's metre to $m$ (using a thread local variable)
	\item \verb'with_metre('$m$\mintinline{ruby}|) bar do ... end| executes a block of user code with a
specified metre $m$, then restores the thread's original metre
\end{itemize}

The \mintinline{ruby}|bar do ... end| command creates a new Bar object, stores this to a
thread local variable, then executes a block of user code. If the Bar object
still has space remaining after the block has run, the function then sleeps for
the appropriate time.


\subsection{Sound commands} \label{sound_commands}

There are three main sound commands: \verb'add_note', \verb'add_sample', and \verb'add_rest'. A
user can use \verb'add_note' to play a note on the current synthesiser. A note is
defined as a pitch (how high or low it sounds) and a length. The pitch is
specified as in Sonic Pi's \verb'play' command, such as by a note name and octave (C4)
or a MIDI note (60). A list of pitches will sound together as a chord. The
length is given as a metrical level and a duration, as defined in Section~\ref{bar_class}.

The \verb'add_note' command works as follows:
\begin{enumerate}
    \item Gets the current Bar object from the thread local variables
    \item Calls the Bar's \verb'add_note()' method to check if the note will fit into the current bar
    \item Passes the note pitch to Sonic Pi's \verb'play' function which creates the sound
    \item Sleeps for the duration of the note
\end{enumerate}

I have also implemented some shorthand commands such as \verb'add_quarter' and
\verb'add_whole' which act as wrappers around calls to \verb'add_note'. This was done to
provide an easy-to-use alternative to the metrical level notation for users who
are less familiar with Western music theory. This was especially important as
Sonic Pi was designed to be accessible to schoolchildren \cite{aaron2013}.

The final two sound commands are \verb'add_sample' and \verb'add_rest'. These function the
same as \verb'add_note', except \verb'add_sample' plays an audio sample and \verb'add_rest' simply sleeps.



\section{Micro-timing} \label{micro-timing_implementation}

In order to add micro-timing functionality to my implementation, I first needed
a way of representing and storing the micro-timing information for different
musical styles. This information is represented by probability distributions for
each event in a metric cycle, describing how early or late each event should
occur. In this section, I will describe how the
Style class stores the micro-timing information for a musical style, and how
this is used to apply micro-timing to a user's music.


\subsection{Style class} \label{style_class}

The Style class stores micro-timing values as a Hash from metrical levels (0, 1,
etc.) to lists containing one probability distribution object for each event at
that level. It will accept any distribution object which has a \verb'sample()' method,
such as my NormalDistribution class described below.

For example, a Style object with the Hash shown below defines micro-timing only for metrical level
$l=0$ (the beat level) for a metre with three beats. The first beat is played exactly on time,
with no variance. The second beat is early, with normal distribution $(\mu,\sigma)=(-0.3,0.01)$.
The third beat is very slightly late, with normal distribution $(\mu,\sigma)=(0.001,0)$.

\begin{minted}{ruby}
{
    0 => [
        NormalDistribution.new(0,0),
        NormalDistribution.new(-0.3, 0.01),
        NormalDistribution.new(0.001,0)
    ]
}
\end{minted}

The class also has a test to see if the style is compatible with a given metrical
hierarchy. A Style is defined as being compatible with a MetreTree if, for each
metrical level defined in the Style, it has exactly one distribution for each
metric event at that level in the MetreTree.

\subsubsection{NormalDistribution class} \label{normal_distribution}

I have implemented a NormalDistribution class, which is initialised with a mean $\mu$ and standard deviation
$\sigma$, and has a method for generating random samples.

Samples are generated using the
Box-Muller transform \cite{box1958}, which transforms a pair of uniform random
samples into a pair of normally distributed samples. Sonic Pi's random number
generator is used for the uniform random samples. This is because it generates a
deterministic, repeatable sequence of pseudorandom numbers, which means the output of
a Sonic Pi program sounds the same each time it is run \cite{aaron2016}.


\subsection{Applying micro-timing} \label{applying_micro-timing}

When a user sets a metre with the \verb'use_metre' or \verb'with_metre' commands, they can
specify an optional second argument as either a Style object or the name of a
style to be looked up. This causes all music played with that metre to use the
micro-timing of the chosen style.

At the start of each new bar, the Metre object samples new values from the
Style's probability distributions. When a note is played inside the bar, the
\verb'add_note' command requests the timing shift that should be applied to the note
from the Metre. To calculate this, the Metre object calls its
\verb'metrical_level_indices()' method (see Section~\ref{metrical_level_indices}) to determine which timing
values from each level to use. The individual timing contributions of each
metrical level are summed to produce an overall timing shift for the
note. A positive value means the note should be played slightly late; a negative
value means slightly early. This is returned to \verb'add_note' which then uses Sonic Pi's \verb'time_warp' function to adjust the timing of the call to \verb'play'.

For example, if the sampled timings for each level are:
\[ T=[0\Rightarrow[0,0.1],1\Rightarrow[0.03,0,0,-0.02]] \]
and the metrical level indices are $L=[0\Rightarrow1,1\Rightarrow3]$, then the timing shift would be
calculated by Equation~\ref{eq:micro-timing_sum_example}:

\begin{equation}
    \begin{split}
        t &= \sum_{i \in T.\mathrm{keys}} T[i][L[i]] \\
        &=T[0][L[0]]+T[1][L[1]] \\
        &=0.1+(-0.02) \\
        &=0.08
    \end{split}
    \label{eq:micro-timing_sum_example}
\end{equation}
Therefore, the note will be played 0.08 quarter lengths late.



\section{Multi-threaded synchronisation} \label{multi-threaded_synchronisation}

An important part of producing music in Sonic Pi is the ability to have multiple
instruments/parts playing simultaneously. This is accomplished by having
separate threads of execution. Aaron et al.'s previous work on Sonic Pi's
temporal semantics \cite{aaron2014} ensures music in separate threads remains in time.
Because of the probabilistic nature of my micro-timing implementation, each
thread needs to have the same set of randomly generated timing values to remain
perfectly synchronised. Otherwise, notes in different threads that are supposed
to sound at the same moment might not do so. To tackle this, I implemented a
SynchronisedMetre class which generates new random timing values exactly once
per bar.


\subsection{SynchronisedMetre class} \label{synchronised_metre}

The SynchronisedMetre class is a subclass of Metre which is designed to control
the synchronisation of all the notes contained in the metre. Its main functions,
in addition to those of the Metre class, are to:
\begin{itemize}
	\item Set a Style to use for generating micro-timing values
	\item Sample from the Style's probability distributions to get timing values
	\item Compute the total timing shift to be applied to a note at a given offset
	\item Ensure all Bar objects which use it remain synchronised and all have the same set of timing values
\end{itemize}


\subsection{Bar number synchronisation} \label{bar_number_synchronisation}

To make sure Bar objects in separate threads all have the same micro-timing
values, and that these values are regenerated exactly once per cycle, I
synchronise the Bars on the current `bar number'. This is a counter of the
number of bars that have occurred since the beginning of playback. The behaviour
of the Bar class is as follows:

\begin{minted}{ruby}
class Bar

    def initialize()
        previous_bar_number = __thread_locals.get(:sonic_pi_bar_number)
        if previous_bar_number
            current_bar_number = @metre.request_bar(previous_bar_number + 1)
        else
            current_bar_number = @metre.request_bar(0)
        end
        __thread_locals.set(:sonic_pi_bar_number, current_bar_number)
    end
end
\end{minted}

If this is the first bar in the thread, the Bar object will request bar number 0
from the metre, otherwise it will request the next bar number in sequence. The
metre returns the actual current bar number, which is then stored in the thread
for the next bar to use.

\begin{minted}{ruby}
class SynchronisedMetre < Metre

    def initialize()
        @timings = recalculate_timings()
        @current_bar_number = 0
        @mutex = Mutex.new()
    end

    def request_bar(requested_bar_number)
        @mutex.synchronise do
            if requested_bar_number > @current_bar_number
                @timings = recalculate_timings()
                @current_bar_number = requested_bar_number
            end
        end
        return @current_bar_number
    end
end
\end{minted}

The SynchronisedMetre's \verb'request_bar()' method determines the correct current bar
number. If the requested number is greater than its internal current number,
then it must be the start of a new bar. So, the timings are recalculated, and
its current bar number is updated. If the requested number is old, then it just
returns the current number. The method is synchronised on a mutex to avoid any
race conditions which might cause bars to get different timing values.

\usetikzlibrary{positioning}
\begin{figure}[ht]
    \centering
    \begin{tikzpicture}[node distance=4]
        \node[] (Metre) {SynchronisedMetre};
        \node[right = of Metre] (Thread1) {Thread 1};
        \node[right = of Thread1] (Thread2) {Thread 2};
        \node[below of=Metre, node distance=7cm] (Metre_ground) {};
        \node[below of=Thread1, node distance=7cm] (Thread1_ground) {};
        \node[below of=Thread2, node distance=7cm] (Thread2_ground) {};
        %
        \draw (Metre) -- (Metre_ground);
        \draw (Thread1) -- (Thread1_ground);
        \draw (Thread2) -- (Thread2_ground);
        \draw[->] (Thread1 |- 0,-1) -- node[above,sloped,midway]{requestBar(0)} node[at start,right]{0} node[at end,left]{0} (Metre |- 0,-2);
        \draw[->] (Metre |- 0,-2) -- node[above,sloped,midway]{$b=0$} node[at end,right]{0} (Thread1 |- 0,-2.5);
        \draw[->] (Thread1 |- 0,-3) -- node[above,sloped,midway]{requestBar(1)} node[at end,left]{1} (Metre |- 0,-4);
        \draw[->] (Metre |- 0,-4) -- node[above,sloped,midway]{$b=1$} node[at end,right]{1} (Thread1 |- 0,-4.5);
        \draw[->] (Thread2 |- 0,-5) -- node[above,sloped,midway]{requestBar(0)} node[at start,right]{0} node[at end,left]{1} (Metre |- 0,-6);
        \draw[->] (Metre |- 0,-6) -- node[above,sloped,midway]{$b=1$} node[at end,right]{1} (Thread2 |- 0,-6.5);

    \end{tikzpicture}
    \caption{Sequence diagram demonstrating the behaviour of bar number synchronisation with multiple threads. Values along the vertical axes show that thread's bar number counter. Arrows represent method calls and their return values.}
    \label{fig:multi-threading_sequence_diagram}
\end{figure}

Figure~\ref{fig:multi-threading_sequence_diagram} is a sequence diagram demonstrating the behaviour of the synchronisation
in the presence of multiple threads. Each object/thread's bar number counter is
shown along its axis. Each thread's counter is initialised to 0. Thread 1
requests bar 0, which is equal to the SynchronisedMetre's current value, so $b=0$
is returned. Thread 1 then requests the next bar number (1). This is greater
than the SynchronisedMetre's current value, so its value is updated and returned.
Thread 2 then starts late and requests an old bar number (0). The
SynchronisedMetre recognises this is old, so simply returns its current value
$b=1$ so Thread 2 can catch up.



\section{Jembe data analysis} \label{jembe_data_analysis}

Creating music with realistic micro-timing using the implementation I have
described requires a set of probability distributions which accurately
characterise a style of music. Therefore, these distributions must be derived
from real-life examples of music. The next two sections describe the data
collection and analysis that was done for the jembe and Viennese waltz styles.
The results of the data analysis methods described here can be found in
Section~\ref{data_analysis_results}.


\subsection{Datasets} \label{jembe_datasets}

Previous research into the micro-timing of Malian jembe music has meant there
are some high-quality datasets of processed live recordings available. See Appendix~\ref{appendix_datasets_jembe} for more performer and recording details.

The first dataset is from Jacoby et al.\ \cite{jacoby2021} and consists of 11
processed recordings of a piece called `Suku', which is a very commonly played
piece in this style. The recordings were made by Rainer Polak in Bamako, Mali in
2016 \cite{jacoby2021supp}.

The second dataset is from the Interpersonal Entrainment in Music Performance
(IEMP) Data Collection \cite{polak2020,clayton2021}. This consists of 15 recordings
across three different pieces: `Manjanin', `Maraka', and `Woloso'. The
recordings were again made by Polak in Bamako in 2006 and 2007.

The datasets supply the following data:
\begin{itemize}
	\item Onset time -- the time when a drum stroke occurs
	\item Cycle number
	\item Metric location -- which metric event the onset belongs to
	\item Phase -- the actual timing of the onset within the cycle
\end{itemize}


\subsection{Micro-timing estimation} \label{jembe_micro-timing}

The pieces of jembe music in the dataset use a metre with four beats, each of
which divides into three, for a total of 12 metric events at the first division
level (often notated as \setmetre{12}{8}). It is at this level, referred to as the `pulse', that the micro-timing occurs.

Each event at the pulse level has a position in the cycle at which a note would
occur if it were isochronous. The offset from this position describes the
micro-timing of a note, and this is what is stored in the probability
distributions described in Section~\ref{style_class}. Equation~\ref{eq:offset_from_phase} shows how the offset is calculated from the phase given by the datasets.

\begin{equation}
    \textnormal{offset} = \frac{(\textnormal{phase} \times \textnormal{beat division}) - \textnormal{metric location}}{2}
    \label{eq:offset_from_phase}
\end{equation}

The phase is multiplied by the beat division (in this case, 3) to convert it
into pulse units. The metric location at the pulse level (an integer from 0 to 11)
is subtracted to get the offset. The final division by two converts the offset
into quarter lengths (because each pulse unit is an eighth length).

Once the offset has been calculated for each drum stroke, I was then able to
estimate the distribution of offsets for each of the twelve metric locations. I
considered two types of approach for this probability density estimation:

\begin{itemize}
	\item Kernel density estimation (non-parametric) -- applies smoothing to the
data to fit an arbitrary distribution
	\item Maximum likelihood estimation (parametric) -- choose an existing
distribution to fit and estimate the parameters which best fit the data
\end{itemize}

The advantage of KDE is that it would be able to accurately represent any shape
of distribution the data may have. However, each point in
the probability density function has to be stored individually. This means it generally has a higher memory
requirement than MLE, which only stores the values of the parameters. Another
advantage of MLE is it allows hypothetical distributions to be easily defined
without the need for data.

For this scenario, I chose to use MLE to fit a normal distribution to the data.
A normal distribution was suitable because the data was assumed to be estimating
a theoretical true value of the offset. A normal distribution requires two
parameters: the mean, $\mu$, and the standard deviation, $\sigma$. It can be shown that the
maximum likelihood estimator $\hat{\mu}$ for the population mean is the sample mean $\bar{x}$,
and the estimator $\hat{\sigma}$ for the population standard deviation is the sample
standard deviation $s$ \cite{dekking2005}. The calculation of $s$ uses Bessel's
correction to get an unbiased estimator of $\hat{\sigma}$ \cite{upton2014}.


\subsection{Tempo estimation} \label{tempo_estimation}

Generating a synthetic piece of jembe music requires analysis of other musical
features as well as the micro-timing to sound realistic. One of these is the
tempo (how fast or slow the beat is), which is measured in beats per minute
(bpm). The tempo of a jembe piece of music typically increases substantially
over the duration of the performance \cite{jacoby2021}, with the last 15 seconds or
so showing the tempo increasing at a much faster rate.

The inter-beat interval (IBI) is defined as the time between two subsequent beats in a
piece of music, from which the instantaneous tempo can be calculated \cite{dixon2001}.
A moving average can be applied to the instantaneous tempo to obtain an estimate
of the global tempo.

For the jembe data, I first filtered all the onsets to include just those played
by Jembe 2 (because it plays on each beat \cite{jacoby2021}), then filtered these to
just onsets on the beats. I then calculated the inter-beat interval using Equation~\ref{eq:ibi}, where $b_i$ is the onset time of the $i$th beat. This is then converted to bpm by Equation~\ref{eq:tempo}. A moving average with window size 10 is then applied to smooth the tempo estimate.

\begin{equation}
    \mathrm{IBI}_i = b_i - b_{i-1}
    \label{eq:ibi}
\end{equation}
\begin{equation}
    t_i = \frac{60}{\mathrm{IBI}_i}
    \label{eq:tempo}
\end{equation}

Inspection of the smoothed tempo graphs showed a logarithmic trend for the first $\sim95\%$ of the piece. A sharper increase follows this which was
modelled by a quadratic curve. To fit curves to the data, I used the \verb'optimize.curve_fit' function from the SciPy Python library, which uses a non-linear least
squares method. The parameters estimated by the curve fitting are then used in Sonic Pi
to control the tempo of a synthetic jembe piece during playback.


\subsection{Rhythm patterns} \label{rhythm_patterns}

The final data analysis performed on the jembe music was an analysis of the
rhythm patterns played by Jembe 1. While the other instruments in the ensemble
play repetitive accompaniments, Jembe 1 has a lead role and can play a variety
of rhythmic patterns, also often involving improvisation. By analysing these
patterns, I was able to produce a synthetic Jembe 1 part for Suku with
semi-realistic rhythms. My approach was as follows:

\begin{enumerate}
    \item Compute the rhythm played by Jembe 1 in each cycle as a 12-bit binary number, where the bit value at each position indicates whether there was an onset at the corresponding pulse in that cycle
    \item Convert the 12-bit binary numbers into a unique decimal integer for each cycle
    \item Count the number of times rhythm $x$ is followed by rhythm $y$ and calculate transition probabilities from these
    \item Generate a random sequence of rhythm integers using the transition probabilities
    \item During playback, for each cycle, play a drum stroke on pulse $i$ if there is a 1 at bit $i$ in the binary representation of that cycle's rhythm integer
\end{enumerate}

This approach was an improvement over purely random rhythms because it preserved
intra-cycle patterns. The jembe can be played with three different techniques,
each producing a different kind of sound (timbre). These are tone, slap, and
bass. To include this, I created a roughly typical pattern covering each note
position in the cycle: [T,T,S,T,T,S,B,B,S,B,B,S]. If a note is played at a
particular pulse, the corresponding timbre is looked up from this pattern.



\section{Waltz data analysis} \label{waltz_data_analysis}

The Viennese waltz provides a useful comparison to Malian jembe in evaluating
this project's micro-timing implementation. This style uses a metre with three
beats (\setmetre{3}{4}) and the micro-timing can be observed at the beat level, where the
second beat is usually early. The micro-timing in Viennese waltz has not been
studied in as much detail or as recently as jembe, so there were no existing
datasets of Viennese waltz performances with micro-timing. This meant further
calculations were needed to derive it, which will be explained in this section.


\subsection{Datasets} \label{waltz_datasets}

I first considered using the Ballroom dataset \cite{gouyon2006}, which contains
30-second recordings of 65 pieces of Viennese waltz music, and for which
annotations of the beats exists \cite{krebs2013}. However, when I listened to the
recordings, I didn't notice any micro-timing. To test my suspicions, I performed
the micro-timing analysis described in Section~\ref{waltz_micro-timing} and then conducted a
one-sample \textit{t}-test. This tests whether the sample mean of the offset of the
second beat is statistically significantly different from 0 (the case where
there is no micro-timing). At the 5\% significance level, there were very few
recordings which had any significant micro-timing, so this dataset was deemed
unsuitable.

I then constructed my own dataset comprising of 30-second samples from seven
waltz recordings performed by the Vienna Philharmonic Orchestra, all of which
have noticeable and statistically significant micro-timing. Because this dataset
has no existing beat annotations, the next step was to produce my own.


\subsection{Beat tracking} \label{beat_tracking}

Beat tracking is the process of identifying the locations of beats in an audio
recording of a piece of music. Since the beat level is where the micro-timing in
the Viennese waltz occurs, no additional onset detection was necessary. The beat
tracking could be approached automatically with existing beat tracking
algorithms, or manually.

A variety of automatic beat tracking algorithms were tried, but most didn't
perform well. Many implementations struggled because they expect the beats to be
isochronous, i.e.\ having no systematic micro-timing, as is the case in most
Western music. As a result, they would often skew the detected beats towards
being isochronous, therefore not capturing the micro-timing. Of all these
implementations, the beat tracking in the libfmp Python library \cite{mueller2021} (a
dynamic programming approach from Müller's book on the Fundamentals of Music Processing
\cite{mueller2021b}) performed the best. After some
small manual corrections, the beat onsets were ready for analysis, as described
in Section~\ref{waltz_micro-timing}.

As an alternative, manual beat annotations were created using the Sonic
Visualiser software \cite{cannam2010}. While the micro-timing from this approach was
more prominent, the data had a large variance which led to the generated music
sounding erratic. For this reason, the automatic approach was chosen.


\subsection{Micro-timing estimation} \label{waltz_micro-timing}

To calculate the micro-timing offset of each beat from just the onset time
involves first identifying the start and end of each cycle, estimating the onset
of each beat as if they were isochronous, then finding the difference between this
and the actual onset to get the offset. The full calculation is shown in
Equation~\ref{eq:offset_from_onset}, where $i$ ranges over the indices of all the onsets in the piece.

\begin{equation}
\begin{split}
    \mathrm{metricLocation}_i &= i \;\mathrm{mod}\; 3 \\
    \mathrm{cycleStart}_i     &= \mathrm{onset}_{i-\mathrm{metricLocation}_i} \\
    \mathrm{cycleEnd}_i       &= \mathrm{onset}_{i-\mathrm{metricLocation}_i+3} \\
    \mathrm{cycleDuration}_i  &= \mathrm{cycleEnd}_i - \mathrm{cycleStart}_i \\
    \mathrm{isochronousBeatDuration}_i &= \frac{\mathrm{cycleDuration}_i}{3} \\
    \mathrm{isochronousOnset}_i &= \mathrm{cycleStart}_i + (\mathrm{metricLocation}_i \times \mathrm{isochronousBeatDuration}_i) \\
    \mathrm{offset}_i &= \frac{\mathrm{onset}_i-\mathrm{isochronousOnset}_i}{\mathrm{isochronousBeatDuration}_i} \\
    \mathrm{phase}_i          &= \mathrm{offset}_i + \mathrm{metricLocation}_i
\end{split}
\label{eq:offset_from_onset}
\end{equation}

The data is assumed to have exactly one onset for each beat in the piece and the
duration of a cycle is assumed to be the time between its first beat and the
first beat of the next cycle.

Once the offsets have been derived, maximum
likelihood estimation is used to fit the probability distributions, as described in
Section~\ref{jembe_micro-timing}.



\section{MusicXML converters} \label{musicxml_converters}

MusicXML is a commonly used file format for encoding Western music notation
which is based on XML \cite{good2001}. In preparation for the evaluation, I have also
implemented a pair of converter programs between MusicXML and my extended Sonic
Pi language.

\begin{itemize}
	\item \verb'convert_mxl.py' uses the \verb'music21' Python library to parse an input MusicXML file into Python objects, which are then iterated over to produce Sonic Pi code
	\item \verb'convert_sonicpi.rb' takes an input Sonic Pi file and iterates over it to produce \verb'music21' Python code. This code is then run automatically to generate the final MusicXML output
\end{itemize}

The most interesting part of the converters is the \verb'convert_duration()' algorithm
which converts a note specified by its quarter length duration into a (metrical
level, multiplier) pair. This is essentially the inverse of the Bar class's
\verb'add_note()' method, which converts a metrical level and multiplier into a quarter length duration (see Section~\ref{bar_class}).

The algorithm first tries to find a metrical
level whose value the note's duration divides into. For example, if the metre
has levels made up of durations $\frac{1}{4}$, $\frac{1}{8}$, and $\frac{1}{16}$, then a note with duration $\frac{5}{16}$
can only exist at the third level, so the algorithm would return $(3,5)$.

If no
such level can be found, then deeper metrical levels must be searched, where we
assume they are divided into two each time. The note's duration in quarter
lengths is first converted to a fraction by dividing by 4. The level $l$ and
multiplier $m$ can then be found as in Equation~\ref{eq:duration_to_level_multiplier}, where $\log_2\left(\frac{n_d}{\mathrm{deepestDenominator}}\right)$ is the number of levels deeper we need to
go.

\begin{equation}
    \begin{split}
        \frac{n_n}{n_d} &= \frac{\mathrm{duration}}{4} \\
        l &= \left\lfloor \mathrm{deepestLevel} + \log_2\left(\frac{n_d}{\mathrm{deepestDenominator}}\right) - 1 \right\rfloor  \\
        m &= n_n
    \end{split}
    \label{eq:duration_to_level_multiplier}
\end{equation}
\newpage



\section{Repository overview} \label{repository}

There are two main components to the repository: the Sonic Pi code, and the data analysis code. The structure of the \verb'sonic-pi' directory follows that of the main Sonic Pi source code\footnote{\url{https://github.com/sonic-pi-net/sonic-pi/tree/stable}}, which is not included here. All code in the repository was written by me, except where specified below.

The following is the high-level structure of the repository:
\vspace{5mm}
\dirtree{%
.1 repository/.
.2 sonic-pi/\DTcomment{Code for micro-timing implementation}.
.3 app/server/ruby/.
.4 lib/sonicpi/.
.5 lang/\DTcomment{Sonic Pi language commands}.
.6 sound.rb\DTcomment{My code is lines 4080--4188}.
.6 western\_theory.rb\DTcomment{My code is lines 1118--1146}.
.5 metre/\DTcomment{Classes for metre implementation}.
.6 bar.rb.
.6 distribution.rb.
.6 metre.rb.
.6 style.rb.
.4 test/.
.5 test\_metre.rb\DTcomment{Unit testing for metre operations}.
.2 micro-timing-data-analysis/\DTcomment{Code for data analysis}.
.3 data/\DTcomment{Raw data of recordings}.
.3 .gitignore.
.3 analysis.py\DTcomment{Loads data for analysis}.
.3 beattrack.py\DTcomment{Performs beat tracking on an audio file}.
.3 convert\_mxl.py\DTcomment{Converts MusicXML to Sonic Pi}.
.3 convert\_sonicpi.rb\DTcomment{Converts Sonic Pi to MusicXML}.
.3 convert\_transcription.py\DTcomment{Converts jembe transcription text files to Sonic Pi}.
.3 metre.rb\DTcomment{Copy of metre code for converters}.
.3 piece.py\DTcomment{Class for analysing a single piece}.
}





\chapter{Evaluation} \label{evaluation}

This chapter aims to demonstrate the capabilities of my project and prove the
achievement of the success criteria.

I will begin by showing the results of the
data analysis which was conducted as part of the implementation. This is a
crucial step towards being able to produce music with realistic micro-timing
because it provides the data needed to populate the styles' probability
distributions.

Secondly, I will describe the motivation, methods, and results of
a user study I conducted to evaluate how realistic the generated music sounds.
This includes a detailed description of how the study was carried out, how the
stimuli were constructed, and the full results. The section concludes with a
discussion of the results and their significance.

Finally, I use the MusicXML
converters implemented earlier to demonstrate the value gained by an
implementation of metre within Sonic Pi and showcase its successful use.



\section{Data analysis results} \label{data_analysis_results}

This section displays the results of the data analysis described in Sections~\ref{jembe_data_analysis} and~\ref{waltz_data_analysis}. I
begin by describing the data-derived micro-timing for jembe and Viennese waltz,
and the patterns which occur in them. Jembe is found to have a short-medium-long
pattern in its pulses, and the waltz has a significantly early second beat. I
then describe the results of the jembe tempo estimation.


\subsection{Micro-timing estimation} \label{micro-timing_estimation_results}

Figure~\ref{fig:suku_histogram} shows the results of the micro-timing estimation (Section~\ref{jembe_micro-timing}) for one of
the jembe pieces, `Suku'. The histograms show the positions within the cycle
that each of the 12 pulses occurred at (phase). The dashed lines show where the
event would occur if they were isochronous, so the existence of the micro-timing
can be seen clearly by the positions of the second and third pulses in each beat.
By examining the positions of the histograms, the length of each pulse can be
seen to follow a short-medium-long pattern (SML), which is consistent across
each beat. Also shown are the probability density functions of the
maximum-likelihood estimated normal distributions. The plots for
the other jembe pieces showed the same pattern.

\begin{figure}[ht]
    \centering
    \input{figures/suku_histogram.pgf}
    \caption{A histogram plot of the positions of each pulse within the cycle for Suku. The histograms are coloured by which beat they belong to. Dashed lines show the metrical grid. Black curves show the PDF of the MLE-derived probability distributions.}
    \label{fig:suku_histogram}
\end{figure}

Figure~\ref{fig:waltz_histogram} shows the results for the waltz dataset (Section~\ref{waltz_data_analysis}). The calculations
use the first beat as the definition for the start of the cycle, so every beat 1
has an offset of 0, which is why its histogram has no variance. The early onset
of the second beat can be clearly seen in the plot ($\mu=-0.0743$, $\sigma=0.0795$). A
one-sample \textit{t}-test confirms the micro-timing is significant ($t=-16.5$, $p=0.000$). Beats 1 and 3 show no significant deviation from the metrical grid, so a short-long-long pattern (SLL) is observed.

\begin{figure}[ht]
    \centering
    \input{figures/waltz-histogram.pgf}
    \caption{A histogram plot of the offset of each beat for the waltz dataset. Dashed lines show the metrical grid. Black curves show the PDF of the MLE-derived probability distributions.}
    \label{fig:waltz_histogram}
\end{figure}


\subsection{Jembe tempo estimation} \label{jembe_tempo_estimation_results}

Figure~\ref{fig:suku_tempo} shows the change in estimated tempo over the duration of the piece for
`Suku', as described in Section~\ref{tempo_estimation}. The tempo starts at around 135 bpm at the beginning of the piece and
ends at around 175 bpm. The dark blue line shows the smoothed mean tempo, and
the black lines show the logarithmic and quadratic curves fit to the data. The
logarithmic curve in this example has the equation $y=17.1 \ln(x+18.7)+83.2$.
The quadratic curve has the equation $y=-0.42x^2+84.2x-4014$. The
shaded area shows $\pm1$ standard deviation from the mean.

The results show the increase in tempo throughout the piece that is characteristic of Malian jembe music. The more dramatic speedup at the end is also reflected in this data -- this is why I fit two different curves to the data. The tempo results match those found by Jacoby et al.\ \cite{jacoby2021supp}, and each jembe piece showed the same trend.

\begin{figure}[ht]
    \centering
    \input{figures/suku-tempo.pgf}
    \caption{Tempo estimation for `Suku'. Grey: Smoothed estimated tempo for each recording. Blue: Mean estimated tempo averaged over all recordings. Shaded: One standard deviation from the mean. Black: Curves fit to the data.}
    \label{fig:suku_tempo}
\end{figure}



\section{User study} \label{user_study}

The key aim of this project was to be able to produce realistic style-specific
micro-timing in Sonic Pi. To evaluate the realism, I have conducted a user study
focusing on the two case study styles: Malian jembe and Viennese waltz. The
methods used in this study were partly based on Neuhoff, Polak, and Fischinger's
previous work \cite{neuhoff2017} which studied experts' perception and judgement of
micro-timing in Malian jembe music.

Participants were first primed on real-life
examples of music from each style, then were presented with five synthetic
samples of music from each style with different micro-timings. They were asked
to score each sample on how well it fit the style, focusing on the timing.

Significant results were obtained for the waltz section, however participants
struggled more with the jembe section. Despite this, an expert in the Malian
jembe style provided comments supporting the success of the implementation. Section~\ref{user_study_discussion} discusses the results in more detail.


\subsection{Method} \label{user_study_method}

To conduct the study, the following two hypotheses were tested on each style:

\begin{enumerate}
    \item \label{hypothesis_1} Participants think samples with the data-derived micro-timing (the ``correct'' timing) fit the style best
    \item \label{hypothesis_2} Participants with more musical experience are more likely to think the correct timing fits best than those with less experience
\end{enumerate}

These hypotheses being accepted for a style would provide evidence for my
implementation being able to produce music with realistic micro-timing.

Micro-timing patterns in this study are given as strings representing the
relative length of each beat/pulse as short (S), medium (M), or long (L). The
data analysis results (Section~\ref{micro-timing_estimation_results}) showed an SML pattern for the micro-timing in
jembe music, and an SLL pattern for the Viennese waltz. Therefore, we would
expect these ``correct'' patterns to be scored highest if the hypotheses are
true.

\subsubsection{Participants} \label{participants}

A total of 23 participants were recruited, made up of 16 musicians and 7
non-musicians, which included a variety of ages and genders. Participants were
recruited mainly through personal connections, and took part by completing an
online form, which provided anonymity and required explicit consent to fill out.
Ethics approval was granted prior to recruitment.

\subsubsection{Priming} \label{priming}

Participants were not assumed to have any prior knowledge of the two musical
styles, so priming was necessary to familiarise them with the styles and their
micro-timing strategies. This meant expertise in the styles was not required, so
participants could be recruited much more easily.

For each of the two styles, participants were given a brief description of the
style and 3--4 examples of real-life performances of music in that style. For
jembe, these consisted of three short excerpts from Polak's 2006 recording of
Manjanin \cite{polak2010}, and the full recording itself. For the waltz, the examples consisted of
30-second recordings of three different pieces by Johann Strauss I \& II
performed by the Vienna Philharmonic Orchestra.

\subsubsection{Scoring} \label{scoring}

Participants were then presented with five synthetic samples for each style,
each with different micro-timing (see Section~\ref{stimulus_construction} below). They were then asked to consider each stimulus and assess how
well they thought it fit with the priming examples they heard. Scores were given
on a scale from 1 (``it sounds completely different'') to 5 (``it sounds fully
identical'').


\subsection{Stimulus construction} \label{stimulus_construction}

The five stimuli for each style were short audio clips of typical music from that style. Each stimulus was produced with a different micro-timing pattern, and this was the only feature of the music which varied between stimuli within each style.

\subsubsection{Jembe} \label{jembe_stimuli}

Following Neuhoff et al.\ \cite{neuhoff2017}, the jembe stimuli consisted of a
four-cycle loop of a typical phrase from the piece `Manjanin'. The stimuli were
generated in Sonic Pi at 145 bpm using jembe drum sound samples provided by
Polak. Each stimulus lasted for approximately 10 seconds and began with a
four-beat click track to help participants identify the beats in the music. The
rhythmic pattern used from Manjanin is shown in Table~\ref{table:manjanin_rhythm} and is taken from the IEMP
data collection \cite{polak2020}.

\begin{table}[ht]
    \centering
    \begin{tabularx}{\linewidth}{
        l
        >{\centering\arraybackslash}X
        >{\centering\arraybackslash}X
        >{\centering\arraybackslash}X
        >{\centering\arraybackslash}X
        >{\centering\arraybackslash}X
        >{\centering\arraybackslash}X
        >{\centering\arraybackslash}X
        >{\centering\arraybackslash}X
        >{\centering\arraybackslash}X
        >{\centering\arraybackslash}X
        >{\centering\arraybackslash}X
        >{\centering\arraybackslash}X
        }
        \toprule
        Metric location & 1.1 & 1.2 & 1.3 & 2.1 & 2.2 & 2.3 & 3.1 & 3.2 & 3.3 & 4.1 & 4.2 & 4.3 \\
        \midrule
        Jembe 1 & S &  & T & S &  & B &  & B & T &  & T &  \\
        Jembe 2 & S &  & T & S &  & B & S &  & T & S &  & B \\
        Dundun 1 & O &  & O &  &  & X &  &  & O &  & O &  \\
        Dundun 2 & O &  &  &  &  & O & O &  &  &  &  & O \\
        \bottomrule
    \end{tabularx}
    \caption{A typical rhythmic pattern from the piece `Manjanin'. A letter at a metric location indicates a drum stroke. Metric locations are given as ``beat number.pulse number''. The letter represents the timbre: Tone, Slap, or Bass for the jembe, and open (O) or closed (X) for the dundun.}
    \label{table:manjanin_rhythm}
\end{table}

Table 4 in Neuhoff et al.\ contains data for the relative length of each
pulse for five different timing
patterns. My stimuli used four of these, plus an isochronous pattern, for a
total of five stimuli. The SLL pattern from Neuhoff et al.\ was discarded as they
found experts could not reliably discriminate between this and SML\@. Therefore,
the final selection of timing patterns was SSL, SML, SLM, SLS, and isochronous.

To use this data in my micro-timing implementation, I first needed to convert
the relative pulse lengths into offsets, which I achieved with Equation~\ref{eq:proportion_to_offset}. The
results of my data analysis in Section~\ref{micro-timing_estimation_results} showed an SML pattern, so offsets for this
pattern were obtained from the data analysis instead.

\begin{equation}
    \begin{split}
        \mathrm{phase}_0 &= 0 \\
        \mathrm{phase}_i &= \mathrm{cycleDuration} \times \sum_{j=0}^{i-1} \mathrm{relativePulseLength}_j \\
        \mathrm{offset}_i &= \mathrm{phase}_i - \mathrm{pulseLocation}_i
    \end{split}
    \label{eq:proportion_to_offset}
\end{equation}

Once the offsets were calcualated (or derived from data), the probability distributions for the stimuli could be created, using the offset as the mean. They used the standard deviations that were found in the data analysis to try to maintain the expressivity of the performance.

\subsubsection{Waltz} \label{waltz_stimuli}

The waltz stimuli consisted of a 30-second excerpt of Johann Strauss II's ``The
Blue Danube'' arranged for piano and played on Sonic Pi's piano synthesiser at
190 bpm.

Table 3 in Neuhoff et al.\ contains similar
pulse-length data to their Table 4 (which I used for jembe above) but uses a different
selection of timing patterns designed to test the importance of a short first
pulse. Consequently, these patterns are also suitable for application to the
beat lengths in the waltz. The final selection of timing patterns was SLL, LSS,
SSL, SLS, and isochronous.

Converting these to offsets was done in the same way as for jembe (Equation~\ref{eq:proportion_to_offset}).


\subsection{Jembe results} \label{user_study_jembe_results}

Figure~\ref{fig:jembe_bar_chart_overall} shows a bar chart of the mean goodness-of-fit scores for each timing
pattern for the jembe section. Figure~\ref{fig:jembe_bar_chart_ability} shows the same data broken down by
musical experience. Table~\ref{table:jembe_t-test} shows the results of a two-tailed pairwise \textit{t}-test between the means of pairs of timing patterns. The significance level for the \textit{t}-test was 95\%.

\begin{figure}[ht]
    \centering
    \makebox[\textwidth][c]{%
        \begin{subfigure}{0.59\textwidth}
            \centering
            \input{figures/results-jembe-overall.pgf}
            \caption{Overall}
            \label{fig:jembe_bar_chart_overall}
        \end{subfigure}
        \begin{subfigure}{0.59\textwidth}
            \centering
            \input{figures/results-jembe-ability.pgf}
            \caption{By musical experience}
            \label{fig:jembe_bar_chart_ability}
        \end{subfigure}
    }%
    \caption{Mean goodness-of-fit scores for each timing pattern for jembe. Error bars show the 95\% confidence interval.}
    \label{fig:jembe_bar_charts}
\end{figure}

\begin{table}[ht]
    \centering
    \begin{tabularx}{\linewidth}{
        l
        >{\raggedleft\arraybackslash}X
        >{\raggedleft\arraybackslash}X
        >{\raggedleft\arraybackslash}X
        >{\raggedleft\arraybackslash}X
        >{\raggedleft\arraybackslash}X
        >{\raggedleft\arraybackslash}X
        }
        \toprule
        \multirow{2}{*}{Pair}
        & \multicolumn{2}{c}{Overall}
        & \multicolumn{2}{c}{Musician}
        & \multicolumn{2}{c}{Non-musician} \\
        \cmidrule(lr){2-3} \cmidrule(lr){4-5} \cmidrule(l){6-7}
        & \multicolumn{1}{c}{$t$}
        & \multicolumn{1}{c}{$p$}
        & \multicolumn{1}{c}{$t$}
        & \multicolumn{1}{c}{$p$}
        & \multicolumn{1}{c}{$t$}
        & \multicolumn{1}{c}{$p$} \\
        \midrule
        SML -- SSL & $-0.20$ & $\mathbf{.847}$ & $-0.52$ & $\mathbf{.609}$ & $0.28$ & $\mathbf{.788}$ \\
        SML -- SLM & $-4.60$ & $.000$ & $-3.16$ & $.006$ & $-3.87$ & $.008$ \\
        SML -- Iso & $-3.28$ & $.003$ & $-2.44$ & $.028$ & $-2.83$ & $.030$ \\
        SML -- SLS & $4.23$ & $.000$ & $4.39$ & $.001$ & $1.19$ & $\mathbf{.280}$ \\
        SSL -- SLM & $2.64$ & $.015$ & $1.69$ & $\mathbf{.111}$ & $2.12$ & $\mathbf{.078}$ \\
        SSL -- Iso & $2.51$ & $.020$ & $1.94$ & $\mathbf{.072}$ & $1.51$ & $\mathbf{.182}$ \\
        SSL -- SLS & $4.58$ & $.000$ & $4.20$ & $.001$ & $1.99$ & $\mathbf{.094}$ \\
        SLM -- Iso & $-0.24$ & $\mathbf{.814}$ & $-0.52$ & $\mathbf{.609}$ & $0.55$ & $\mathbf{.604}$ \\
        SLM -- SLS & $2.40$ & $.025$ & $3.04$ & $.008$ & $-0.42$ & $\mathbf{.689}$ \\
        Iso -- SLS & $2.13$ & $.045$ & $2.67$ & $.017$ & $0.00$ & $\mathbf{1}$ \\
        \bottomrule
    \end{tabularx}
    \caption{Results of a two-tailed pairwise \textit{t}-test between the means of pairs of timing patterns for jembe. \textit{p}-values which are not significant at the 95\% level are in bold.}
    \label{table:jembe_t-test}
\end{table}

The patterns with the greatest overall mean scores were SML ($\mu=3.57$) and SSL
($\mu=3.52$). Despite SML scoring slightly higher, the \textit{t}-test found the difference
to be insignificant. When looking just at musicians, there was a greater
difference between SML and SSL, but it was still insignificant. Both SML and SSL
were scored significantly higher than all other patterns. Therefore, despite
there being some suggestion that Hypothesis~\ref{hypothesis_1} is true for jembe, there is
insufficient evidence to accept it.

The pattern with the lowest overall mean score was SLS ($\mu=2.35$), which was
significantly lower than all other patterns. This is consistent with Neuhoff et
al.'s findings. The SLM and isochronous patterns received very similar overall
mean scores such that the \textit{t}-test found there to be no significant difference
between them.

Musicians gave a higher mean score to SML than non-musicians, however a \textit{t}-test
between musicians' and non-musicians' scores found this difference to not be
significant ($t=1.05, p=0.152$). Therefore, despite there also being some suggestion
that Hypothesis~\ref{hypothesis_2} is true for jembe, there is insufficient evidence to accept it.


\subsection{Waltz results} \label{user_study_waltz_results}

Figure~\ref{fig:waltz_bar_charts} and Table~\ref{table:waltz_t-test} show mean goodness-of-fit scores and \textit{t}-test results for the
waltz section of the study.

\begin{figure}[ht]
    \centering
    \makebox[\textwidth][c]{%
        \begin{subfigure}{0.59\textwidth}
            \centering
            \input{figures/results-waltz-overall.pgf}
            \caption{Overall}
            \label{fig:waltz_bar_chart_overall}
        \end{subfigure}
        \begin{subfigure}{0.59\textwidth}
            \centering
            \input{figures/results-waltz-ability.pgf}
            \caption{By musical experience}
            \label{fig:waltz_bar_chart_ability}
        \end{subfigure}
    }%
    \caption{Mean goodness-of-fit scores for each timing pattern for Viennese waltz. Error bars show the 95\% confidence interval.}
    \label{fig:waltz_bar_charts}
\end{figure}

\begin{table}[!ht]
    \centering
    \begin{tabularx}{\linewidth}{
        l
        >{\raggedleft\arraybackslash}X
        >{\raggedleft\arraybackslash}X
        >{\raggedleft\arraybackslash}X
        >{\raggedleft\arraybackslash}X
        >{\raggedleft\arraybackslash}X
        >{\raggedleft\arraybackslash}X
        }
        \toprule
        \multirow{2}{*}{Pair}
        & \multicolumn{2}{c}{Overall}
        & \multicolumn{2}{c}{Musician}
        & \multicolumn{2}{c}{Non-musician} \\
        \cmidrule(lr){2-3} \cmidrule(lr){4-5} \cmidrule(l){6-7}
        & \multicolumn{1}{c}{$t$}
        & \multicolumn{1}{c}{$p$}
        & \multicolumn{1}{c}{$t$}
        & \multicolumn{1}{c}{$p$}
        & \multicolumn{1}{c}{$t$}
        & \multicolumn{1}{c}{$p$} \\
        \midrule
        SLL -- Iso & $-2.39$ & $.026$ & $-4.28$ & $.001$ & $0.81$ & $\mathbf{.448}$ \\
        SLL -- SSL & $-6.15$ & $.000$ & $-5.22$ & $.000$ & $-3.29$ & $.017$ \\
        SLL -- LSS & $-7.13$ & $.000$ & $-6.26$ & $.000$ & $-4.58$ & $.004$ \\
        SLL -- SLS & $-6.26$ & $.000$ & $-8.47$ & $.000$ & $-1.44$ & $\mathbf{.200}$ \\
        Iso -- SSL & $1.44$ & $\mathbf{.165}$ & $0.00$ & $\mathbf{1}$ & $3.06$ & $.022$ \\
        Iso -- LSS & $2.86$ & $.009$ & $1.58$ & $\mathbf{.135}$ & $2.97$ & $.025$ \\
        Iso -- SLS & $-3.32$ & $.003$ & $-1.96$ & $\mathbf{.068}$ & $-4.50$ & $.004$ \\
        SSL -- LSS & $2.24$ & $.036$ & $2.24$ & $.041$ & $0.55$ & $\mathbf{.604}$ \\
        SSL -- SLS & $-2.12$ & $.045$ & $-3.47$ & $.003$ & $0.00$ & $\mathbf{1}$ \\
        LSS -- SLS & $-0.40$ & $\mathbf{.692}$ & $-0.90$ & $\mathbf{.383}$ & $0.26$ & $\mathbf{.805}$ \\
        \bottomrule
    \end{tabularx}
    \caption{Results of a two-tailed pairwise \textit{t}-test between the means of pairs of timing patterns for Viennese waltz. \textit{p}-values which are not significant at the 95\% level are in bold.}
    \label{table:waltz_t-test}
\end{table}

The timing pattern with the greatest overall mean score was SLL ($\mu=3.83$), which
the \textit{t}-test found to be significantly higher than any other pattern. Therefore,
Hypothesis~\ref{hypothesis_1} is supported by the data for the waltz style. When looking
at musicians, this same trend can be seen, with SLL being scored much higher
than all others. Non-musicians, however, scored the isochronous pattern higher
than SLL, although the difference wasn't statistically significant.

The isochronous pattern had the second-highest overall mean score ($\mu=3.13$),
although this was not significantly different to SSL ($\mu=2.74$).

The patterns with the lowest overall mean scores were SLS ($\mu=2.26$) and LSS ($\mu=2.
35$), and the difference between them was not significant. This is also the case
when looking at musicians and non-musicians individually.

A one-tailed \textit{t}-test between musicians' and non-musicians' scores found that
musicians were significantly more likely to think SLL was a good fit than
non-musicians ($t=2.07, p=0.025$). It similarly found that musicians were
significantly less likely to think the isochronous pattern was a good fit than
non-musicians ($t=-2.05, p=0.027$). Therefore, Hypothesis~\ref{hypothesis_2} is supported by the
data for the waltz style.


\subsection{Discussion} \label{user_study_discussion}

The results for the Viennese waltz style have shown that my implementation is
capable of producing music with realistic micro-timing, with participants
showing a significant preference for the SLL timing pattern which used
data-derived micro-timing. The preference is particularly clear in musicians,
which is expected as they have more experience in this area.

Non-musicians showed a preference (albeit a statistically insignificant one) for the
isochronous pattern for the waltz. This is not surprising, as the participants
are most likely to be familiar with Western popular music, which is typically
isochronous \cite{soley2010}. This suggests that many non-musicians may have failed to pick up on
the micro-timing in the priming examples, and instead scored the stimuli based
on what sounded most familiar to them.

The results for the jembe style were generally not significant enough to prove
the two hypotheses. However, the data-derived SML pattern still ended up being
the joint-highest scoring pattern as expected.

One of the main reasons I believe caused participants to struggle more with the
jembe was that the micro-timing is at a deeper metrical level, so the three
pulses making up the SML pattern occur in a very short space of time. In fact, all three pulses are
played in about 0.4 seconds, compared to about 1 second for the three beats of
the waltz. This meant that participants unfamiliar with the style would likely
find it much harder to identify the micro-timing, due to its speed. This is also
supported by the fact that jembe had a lower mean familiarity score than the
waltz (1.70 compared to 2.83).

It is likely that the results for the jembe section would have shown a stronger
trend for the SML pattern if the participants had been more familiar with jembe
music, as in Neuhoff et al.\ \cite{neuhoff2017}.

In addition to this study, I generated a full-length synthetic recording of
Manjanin with the data-derived micro-timing. In addition to providing much
useful advice, Rainer Polak also gave an expert opinion on the full recording,
describing it as ``realistic and beautiful''. Therefore, despite the unfamiliar
participants in the study struggling to identify the expected pattern, I conclude that the implementation can successfully produce realistic music.
\newpage



\section{Metre evaluation} \label{metre_evaluation}

The final method I have used to assess the project is to evaluate the
implementation of metre by demonstrating the new utility that can be gained from
it. To do this, I have implemented a pair of converters which convert between
music represented by MusicXML and by my extended Sonic Pi language. The goal is
to convert a section of music from MusicXML into Sonic Pi and back again without
any loss.

Figure~\ref{fig:musicxml_before} shows a short line of music written in Western music notation,
representing a MusicXML file. This was then converted to Sonic Pi code (shown in
Appendix~\ref{appendix_code_metre_evaluation}), then converted back to MusicXML. The final result is
shown in Figure~\ref{fig:musicxml_after}. The resulting music sounds identical, and the changes in
metre are preserved. The only difference is a slight change in the beaming in
bar 3 due to the \verb'music21' library inferring the beaming groups incorrectly.

With the existing Sonic Pi language, it would not be possible to perform this
round-trip conversion without loss of information. In particular, without an
implementation of metre, information about the metre of the piece and the
grouping of notes into cycles would be lost upon conversion to Sonic Pi.
Therefore a conversion back to MusicXML would have a different result to
the original music.

Figure~\ref{fig:musicxml_bad} shows the results when metre information is lost during conversion. The \verb'music21' library infers an incorrect \setmetre{12}{4} time signature, and the separation of the notes into bars is lost.

\begin{figure}[ht]
    \centering
    \begin{subfigure}{\textwidth}
        \centering
        \includegraphics[width=\linewidth]{figures/metre_eval_orig.pdf}
        \caption{Original}
        \label{fig:musicxml_before}
    \end{subfigure}
    \begin{subfigure}{\textwidth}
        \centering
        \includegraphics[width=\linewidth]{figures/metre_eval_conv.pdf}
        \caption{After round-trip conversion, with metre implementation}
        \label{fig:musicxml_after}
    \end{subfigure}
    \begin{subfigure}{\textwidth}
        \centering
        \includegraphics[width=\linewidth]{figures/metre_eval_bad.pdf}
        \caption{After round-trip conversion, without metre implementation}
        \label{fig:musicxml_bad}
    \end{subfigure}
    \caption{A short line of music stored in a MusicXML file. Shown before and after a round-trip conversion to Sonic Pi and back to MusicXML, with and without a metre implementation.}
    \label{fig:musicxml_before_after}
\end{figure}

In summary, the successful conversion of MusicXML music to Sonic Pi and back
demonstrates the value and successful performance of my metre implementation
because this would not have been possible without it.





\chapter{Conclusions} \label{conclusions}

In this project, I have investigated and implemented probabilistic
style-specific micro-timing in a music live coding language. To do this, I
extended the Sonic Pi language with an implementation of musical metre and
micro-timing, and developed a method of synchronising this micro-timing between
multiple threads. I then performed data analysis on recordings of music from two
case study styles to generate music with realistic micro-timing.

This chapter summarises the key achievements of my project, the lessons learnt,
and some possibilities for future work in this area.



\section{Achievements} \label{achievements}

The primary aim of this project was to extend the Sonic Pi live coding language
to allow users to create music with style-specific micro-timing. This required
the music to be written in a metrical context, which Sonic Pi doesn't have. I
have successfully implemented a functional concept of musical metre, using my
MetreLeaf and MetreTree classes to construct arbitrary metrical hierarchies and
perform transformations on them.

Having metre in Sonic Pi is also useful in its own right as it allows for the
representation and manipulation of metrical structures within a piece of music.
This utility, as well as the correct operation of my implementation, was
demonstrated successfully in the evaluation (Section~\ref{metre_evaluation}).

Probabilistic micro-timing has been implemented by using normal distributions to
define the offset of each metrical event in a cycle from its position on the
grid. Random samples are drawn from these distributions during playback to
adjust the timing of notes.

The second key part of my success criteria was to be able to use this
implementation to generate music with realistic micro-timing. To do this, I
performed data analysis on recordings of music from the Malian jembe and
Viennese waltz styles. This allowed me to derive probability distributions and
generate appropriate music. By including Viennese waltz, I was able to achieve
one of my extension criteria.

I then conducted a user study to evaluate the realism of the synthetic music.
The results of this were significant for the waltz style, which demonstrated my
project's success in achieving this. I also received feedback from an expert in
jembe music, who described the results as ``realistic and beautiful''.



\section{Lessons learnt} \label{lessons_learnt}

This project has given me the opportunity to develop my understanding of areas
of computer science such as data science, distributed systems, and tree data
structures. I have also learnt a lot about micro-timing in music, the jembe
style of music, and the theory of metre.

One thing I would do differently is I would've more carefully designed the metre
implementation before starting. This is because I had to make significant
changes when I added the multi-threaded synchronisation, as this was an
unforeseen problem to solve. Another improvement that could be made would be to
use a more consistent set of priming examples for the Viennese waltz in the user
study (Section~\ref{priming}). The examples that were used exhibited different degrees of
micro-timing, and a more coherent set would have improved the effectiveness of
the priming.



\section{Future work} \label{future_work}

There are a variety of possibilities for future work in this area. Firstly, a
useful extension to my project would be to add a GUI feature to Sonic Pi's IDE
which allows users to define their own micro-timing presets and interact with
the probability distributions directly.

An obvious extension would be to perform the data analysis for a number of
additional styles. Other styles with well-known micro-timing include jazz swing
rhythms \cite{dittmar2018}, candombe drum ensembles from Uruguay \cite{jure2016,fuentes2019}, and Brazilian samba music \cite{naveda2011,fuentes2019}. Furthermore,
creation of a larger dataset of Viennese waltz recordings would be beneficial in
deriving more accurate distributions.

Finally, future work could also involve implementing a control within Sonic Pi
to dynamically adjust the ``strength'' of the micro-timing, i.e.\ transform the
timing offsets between style-specific and isochronous.





\printbibliography[heading=bibintoc]





\appendix





\chapter{Datasets} \label{appendix_datasets}

This appendix details the datasets used in the data analysis (Sections~\ref{jembe_data_analysis} and~\ref{waltz_data_analysis}) and user study (Section~\ref{user_study}).



\section*{Jembe} \label{appendix_datasets_jembe}

Two datasets of recordings of jembe performances were used.

The first is from Jacoby et al. \cite{jacoby2021} and consists of 11 recordings of a piece called `Suku', which is very commonly played in Mali. The recordings were made by Rainer Polak in Bamako, Mali in 2016. They feature Drisa Kone on Jembe 1, Sedu Keita on Jembe 2, and Madu Jakite on Dundun \cite{jacoby2021supp}. The dataset can be accessed from \url{https://osf.io/8wyav/}.

The second dataset is part of the Interpersonal Entrainment in Music Performance (IEMP)
Data Collection \cite{polak2020,clayton2021} and consists of 15 recordings across three different pieces: `Manjanin', `Maraka', and `Woloso'. The recordings were made by Polak in Bamako in 2006/7. They feature performances by Drisa Kone, Sedu Balo, Madu Jakite, Antoine Traole, Isa Coulibaly, and Jeli Madi Kuyate. Each recording is between 2 and 6 minutes in length. The dataset can be accessed from \url{https://osf.io/m652x/}.

For the jembe section of the user study, participants were primed on four recordings of jembe music (Section~\ref{priming}). The first two of these are Audio 1a and 1b from Polak \cite{polak2010}. These are 14-second loops of Cycle 178 from his 2006 recording of Manjanin. Audio 1b isolates the Jembe 1 part from Audio 1a. The third example was a 13-second loop of Cycles 17 and 18 from the same recording. I included this so participants could be shown an example at a slower tempo. The final example was the full recording of Manjanin, which lasted 5 minutes.



\section*{Waltz} \label{appendix_datasets_waltz}

Due to the lack of suitable existing datasets for Viennese waltz discussed in Section~\ref{waltz_datasets}, I created a new dataset consisting of 30-second samples from seven recordings of waltz pieces performed by the Vienna Philharmonic Orchestra. The full dataset is shown in Table~\ref{table:waltz_dataset}. All pieces were composed by Johann Strauss II, except for Recording 4, which was composed by Johann Strauss I. For the waltz section of the user study, participants were primed on Recordings 4, 5, and 7 from the dataset (Section~\ref{priming}). 

\begin{table}[ht]
    \begin{tabularx}{\linewidth}{
        l
        >{\raggedright\arraybackslash}X
        l
        l
    }
        \toprule
        ID & Title & Conductor & Recording date \\
        \midrule
        1 & The Blue Danube & Georges Prêtre & 1st January 2010 \\
        2 & ``Die Fledermaus'' Overture & Zubin Mehta & 29th May 1999 \\
        3 & Kaiser-Walzer & Herbert von Karajan & 1st January 1987 \\
        4 & Loreley-Rhein-Klänge & Zubin Mehta & 29th May 1999 \\
        5 & Tales From The Vienna Woods & Daniel Barenboim & 1st January 2014 \\
        6 & Wiener Blut & Zubin Mehta & 29th May 1999 \\
        7 & Wiener Bonbons & Zubin Mehta & 29th May 1999 \\
        \bottomrule
    \end{tabularx}
    \caption{Table of recordings included in the Viennese waltz dataset}
    \label{table:waltz_dataset}
\end{table}





\chapter{Code examples} \label{appendix_code_examples}

This appendix shows examples of Sonic Pi code that use my implementation to play music. These were used as part of testing and evaluation of the project.



\section*{Manjanin} \label{appendix_code_manjanin}

The following Sonic Pi code is for a full performance of the jembe piece Manjanin, using a transcription of Polak's 2006 recording (see Appendix~\ref{appendix_datasets_jembe}). The rest of the transcribed Jembe 1 part at line 91 has been omitted.

\inputminted[linenos=true]{ruby}{figures/manjanin_code.rb}



\section*{The Blue Danube} \label{appendix_code_blue_danube}

The following Sonic Pi code is for an excerpt of Johann Strauss II's ``The Blue Danube'' arranged for piano. This was used to generate the stimuli for the waltz part of the user study (Section~\ref{waltz_stimuli}). To save space, only the melody is shown in full here.

\begin{multicols}{2}
    \inputminted[linenos=true]{ruby}{figures/blue_danube_code.rb}
\end{multicols}



\section*{Metre evaluation} \label{appendix_code_metre_evaluation}

As part of the evaluation of my metre implementation (Section~\ref{metre_evaluation}), I converted the music shown in Figure~\ref{fig:appendix_musicxml_before} from a MusicXML representation to Sonic Pi code, and back. This appendix shows the intermediate Sonic Pi code generated, first with the metre implementation, and then without.

\begin{figure}[ht]
    \centering
    \includegraphics[width=\linewidth]{figures/metre_eval_orig.pdf}
    \caption{Western music notation representation of the music used in the metre evaluation. Reproduced from Figure~\ref{fig:musicxml_before_after}.}
    \label{fig:appendix_musicxml_before}
\end{figure}


\subsection*{With metre implementation}

\inputminted[linenos=true]{ruby}{figures/metre_eval_code.rb}


\subsection*{Without metre implementation}

\begin{multicols}{2}
    \inputminted[linenos=true]{ruby}{figures/metre_eval_bad_code.rb}
\end{multicols}





\chapter{Project proposal} \label{project_proposal}

\begin{refsection}

\begin{center}
    \Large \bf Part II Project Proposal

    Musical micro-timing for live coding
\end{center}

\section*{Introduction}

The aim of this project is to create a tool to generate music with
style-specific micro-timing integrated into a live coding environment such as
Sonic Pi.

Each style will have a probability distribution for the timing of event
occurrences within a metrical cycle which is used to randomly adjust the exact
timing of each note. This will allow micro-timing found in styles such as
Viennese Waltz, swing, Malian jembe drum ensembles, etc.\ to be used in live
coding.

The Malian jembe piece `Suku' will be used as a primary case study for this
project due to having a highly consistent micro-timing strategy, however the
application will be able to encode and generate music in other styles.

The implementation will be evaluated for its generative power, i.e.\ how
realistic the generated music is for specific musical styles, using human judges
with expertise in these styles, and for its responsiveness.


\subsection*{Metre}

A beat is a regularly occurring pulse in music. Metre describes the ways beats
are grouped and divided. Simple metres divide the beat into two, and then four.
Compound metres divide the beat into three, and then six \cite{omt2021}.

\begin{figure}[ht]
    \includegraphics[width=0.6\linewidth]{figures/proposal/metre example.png}
    \caption{Diagram showing how the beat can be divided into distinct metrical levels in a simple metre.}
    \label{fig:proposal_metre_example}
\end{figure}

Duple metres group two beats together, triple metres group three beats together,
and quadruple metres group four beats together. Beats can be grouped together to
form metrical cycles (bars) \cite{omt2021}.

All music performed by humans deviates from these simple ratios, and the ways in
which these deviations occur contribute significantly to what we call musical
style. Different musical styles have specific micro-timing strategies, such as
the long-short pattern in a swing rhythm, or the short-long-long pattern in a
Viennese Waltz. This micro-timing is not usually explicitly written but the
performers use their knowledge of the style to play the micro-timings, as
described by the Many Metres Hypothesis \cite{london2012}. When transcribed into something
like Western notation much of this timing information is lost.


\subsection*{Live coding}

Music live coding is a way of creating and performing music by writing and
modifying code in real time. A popular music live coding program is Sonic Pi,
which was designed as an educational tool for teaching programming in schools.
It implements its own domain-specific language written in Ruby, using the
SuperCollider sound synthesis server to produce sounds \cite{aaron2013}. One drawback of
Sonic Pi is that it currently doesn't have a built-in notion of metre or
style-specific micro-timing, which is what this project hopes to address.


\subsection*{Case study}

The Malian jembe musical style consists of a small group of drummers with
different instruments and roles playing very fast and complex rhythms. In a
traditional jembe trio, there are two jembe players and one dundun player, with
Jembe 1 having a lead role, Jembe 2 playing a simple accompaniment, and Dundun
playing a varying pattern that's characteristic of each piece of music \cite{jacoby2021}.

This style of music is an ideal case study for this project because it has a
highly consistent micro-timing strategy; Malian drummers have been shown to have
one of the lowest levels of timing variability between performers in the world
\cite{jacoby2021}. It is also relatively constrained in that the pitch, timbre, and number of
instruments is very limited, so timing can be focused on easily.

The specific piece of music I will be looking at is Suku. This is one of around
25 commonly played pieces in this style, out of over 60 in total in Bamako. Suku
is the most frequently played of these pieces so is typical of the style, and
the data gathered by Jacoby et al.\ \cite{jacoby2021} from live performances of this piece will
allow me to produce probability distributions which accurately represent the
micro-timing of this style of music.

Figure~\ref{fig:proposal_histogram} shows the timing of drum-strokes in Suku and its compound quadruple
metre. By looking at the histogram, it can be seen that the metric positions are
non-isochronous because they aren't equally spaced. Figure~\ref{fig:proposal_transcription} shows how this
rhythm might be written in Western notation, but this loses the micro-timing
information.

\begin{figure}[ht]
    \includegraphics[width=\linewidth]{figures/proposal/histograms.png}
    \caption{Onset data for each instrument. The event histogram shows the position of each event within the cycle. Reproduced from \cite{jacoby2021}.}
    \label{fig:proposal_histogram}
\end{figure}

\begin{figure}[ht]
    \includegraphics[width=\linewidth]{figures/proposal/transcription.pdf}
    \caption{Drum-strokes from Suku transcribed into Western notation in a 12/8 time signature. The micro-timing of the music is lost in this notation.}
    \label{fig:proposal_transcription}
\end{figure}



\section*{Starting point}

No work has been undertaken on this project so far. I will be building this on
top of the existing codebase of Sonic Pi, which will handle audio playback and
basic timekeeping.



\section*{Substance and structure}

This project will implement the style-specific micro-timing strategies into
Sonic Pi, using Suku as a case study. This will be built flexibly to allow other
styles to be encoded and combined in creative ways, such as by applying the
micro-timing strategies of one style to music from another style.

The first step will be to gain familiarity with Sonic Pi and be able to produce
some simple music using it. I will then need to investigate how timing works in
Sonic Pi, and gain familiarity with its codebase and the languages it's written
in. Next, I will begin writing code which interacts with or modifies the
functionality of Sonic Pi. To do this, I will need to investigate the best way
of interacting with it, whether this be modifying the Sonic Pi code itself or
writing an application which communicates with it or with the SuperCollider
server.

The next main step will be to decide on a method of encoding probability
distributions for the timing of event occurrences within a metrical cycle at a
particular metrical level, and for each instrument/part in the piece of music.
This then needs implementing in the language chosen in the previous stage.

The final stage of implementation will be to use the representation of the
probability distributions to randomly adjust the timing of each beat (or
equivalent) during execution of a Sonic Pi program to produce the desired
micro-timing effects.

After this has been completed, I will use the data from \cite{jacoby2021} to produce
probability distributions for Suku. The data includes the exact time of each
drum-stroke (onset), its metric position in the cycle, and its phase (exact
offset from the downbeat of the cycle). This is provided for each instrument and
for multiple recording takes. This was used in the paper to produce the graph in
Figure~\ref{fig:proposal_histogram}. I will use maximum likelihood estimation for each of the twelve metric
grid positions for each instrument to fit a normal probability distribution to
the data for each one. The resulting probability distributions will describe the
position of each event within the metrical cycle. I will then encode this using
the representation devised earlier in the project and will use samples of the
jembe and dundun drums to synthetically generate music with the micro-timing of
Suku.

The final steps of the project will be to perform the evaluation (as described
below), and write the dissertation.


\subsection*{Evaluation}

As this project involves generating music, which is inherently subjective,
evaluating this will primarily involve performing user studies.

The generative power of the program can be defined as how realistic the
generated music is for specific musical styles. This can be evaluated by
generating a given piece of music with varying probability distributions and
playing these for users with a strong familiarity with the style. The user will
then be asked to rank them in order of how realistic they seem as pieces of
music within this style, focusing specifically on the micro-timing. The
hypothesis would be that the music with the correct distribution for that style
would seem the most realistic.

For the case study of Suku, Rainer Polak is an expert on Malian jembe music so
would be an ideal user for the study. Due to the nature of this musical style,
expertise is required in the style to judge the accuracy of the micro-timing.

The generative power could also be evaluated for swing rhythms. The triplet
swing (in the ratio 2:1) is a typical swing ratio and an exact distribution can
be derived for this without the need for a dataset. The generative power can
then be evaluated for swing as above. Users could easily be recruited for this
study as most musicians/music students will be familiar with this style of music.
To constrain this to focus on the timing, the stimulus could be just a drumbeat
as this removes the effect of pitch and timbre on the user's judgment. This
maintains good ecological validity because a swung jazz drumbeat will be very
recognisable by users familiar with the style, despite lacking the other
instrumental parts typically found in this style.

As an extension, because musical timing is critical in this project its
responsiveness could also be evaluated. This could be done by measuring the
additional latency caused by adding this functionality to Sonic Pi, for a range
of examples, and ensuring this is sufficiently small as not to be detectable by
humans.



\section*{Extensions/future work}

An extension to this project could be to gather more data to produce probability
distributions for other styles, so their micro-timing strategies can be
reproduced more accurately. In addition to this, further work could be done to
integrate a way for users to interact with the probability distributions and
define their own in a GUI built into Sonic Pi. Another extension could be to
implement controls to increase or decrease style-specificity, allowing the
probability distributions to be continuously transformed from a generic
distribution (with no micro-timing) to a style-specific distribution.



\section*{Success criteria}

This project will be successful if these success criteria are met:

\begin{itemize}
	\item Implemented a representation of probability distributions for
style-specific micro-timing
	\item Implemented a tool to play music in Sonic Pi with probabilistic
micro-timing
	\item Conducted a user study to evaluate how realistic the generated music is
with respect to a specific style
\end{itemize}



\section*{Plan of work}

\begin{itemize}
    \item 16th October -- 5th November (3 weeks)
    \begin{itemize}
    	\item Learn how to use Sonic Pi and get familiar with its language and the languages it's written in
    	\item Start to gain familiarity with Sonic Pi's codebase and its temporal semantics
    	\item \textbf{Be able to create music in Sonic Pi, and understand how its timing works}
    \end{itemize}
    \item 6th November -- 26th November (3 weeks)
    \begin{itemize}
    	\item Learn how Sonic Pi works, particularly how it uses SuperCollider
    	\item Investigate the best way of interacting with Sonic Pi and choose a language to do this in (e.g., a separate application or add to its codebase?)
    	\item \textbf{Be able to write my own code that interacts with or modifies the functionality of Sonic Pi}
    \end{itemize}
    \item 27th November -- 17th December (3 weeks)
    \begin{itemize}
    	\item Investigate the best way of representing, inputting, and storing the probability distributions of metrical event occurrences
    	\item Implement this representation and create some hypothetical distributions for testing
    	\item \textbf{Be able to encode the required probability distributions for a (hypothetical) style of music and be able to retrieve individual probability values}
    \end{itemize}
    \item 18th December -- 7th January (3 weeks)
    \begin{itemize}
    	\item Catch up on work from previous milestones, particularly the last one as this is quite involved
    	\item Break for Christmas, some tidying up of the code can also be done here
    	\item \textbf{Ensure work is up-to-date and sufficient notes of my progress have been made}
    \end{itemize}
    \item 8th January -- 28th January (3 weeks)
    \begin{itemize}
    	\item Investigate how to implement micro-timing in Sonic Pi
    	\item Add a control to use a selected probability distribution to control the micro-timing
    	\item \textbf{Be able to accurately generate music with micro-timing that uses a given probability distribution}
    \end{itemize}
    \item 29th January -- 4th February (1 week)
    \begin{itemize}
    	\item Prepare progress report and presentation (dates TBC so this timing is a guess)
    	\item Catch up on any missed work from the previous milestone
    	\item \textbf{Submit progress report and give presentation}
    \end{itemize}
    \item 5th February -- 18th February (2 weeks)
    \begin{itemize}
    	\item Use the data from \cite{jacoby2021} to create a probability distribution for the Malian jembe piece `Suku' empirically
    	\item Use this and the drum sound samples to synthetically generate this piece of music
    	\item \textbf{Be able to generate music with micro-timing accurate to this style}
    \end{itemize}
    \item 19th February -- 4th March (2 weeks)
    \begin{itemize}
    	\item Prepare music and questions for user studies
    	\item Perform user studies and gather measurements for evaluation
    	\item \textbf{Have completed all the data gathering for evaluation}
    \end{itemize}
    \item 5th March -- 18th March (2 weeks)
    \begin{itemize}
    	\item Analyse data from the evaluation and write up the results
    	\item Add finishing touches and tidy up the implementation
    	\item \textbf{Finalise code and be prepared to begin writing the dissertation}
    \end{itemize}
    \item 19th March -- 22nd April (5 weeks)
    \begin{itemize}
    	\item Use the Easter holidays to write the full dissertation
    	\item Have a draft prepared for the end of the holidays
    	\item \textbf{Submit a draft of the full dissertation}
    \end{itemize}
    \item 23rd April -- 13th May (3 weeks)
    \begin{itemize}
    	\item Act on feedback from the draft dissertation and submit the final version
    	\item Revise for exams
    	\item \textbf{Final dissertation submitted before the deadline}
    \end{itemize}
\end{itemize}



\section*{Resources}

I will use my personal computer for the development of this project. Its
specifications are: Intel Core i7-7700K (4.2 GHz), 16GB memory, with over 600GB
of available storage. In case of failure, I plan to use my personal laptop, or
use the University MCS\@. I have local backups stored to an external hard drive,
monthly backups to Google Drive, and I will use GitHub for version control. I
accept full responsibility for this machine, and I have made contingency plans
to protect myself against hardware and/or software failure.

While not critical to the implementation, the data from \cite{jacoby2021} will be used to
generate probability distributions for the case study piece of music. This is
publicly available at \url{https://doi.org/10.17605/OSF.IO/8WYAV} but I have
downloaded the data anyway.



\printbibliography[heading=subbibliography]
\end{refsection}





\end{document}
